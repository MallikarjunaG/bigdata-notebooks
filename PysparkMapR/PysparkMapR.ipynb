{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7effa1522780>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pyspark-shell'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-1491128901054'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PYTHONHASHSEED': '0'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1491128897782"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notebook'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.sparkUser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pythonVer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.zip', '.egg', '.jar')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.PACKAGE_EXTENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pythonExec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\n",
    "rdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(16) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423 []'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(8) ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423 []'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd1.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HiveContext',\n",
       " 'In',\n",
       " 'Out',\n",
       " 'SQLContext',\n",
       " 'SparkContext',\n",
       " 'StorageLevel',\n",
       " '_',\n",
       " '_1',\n",
       " '_10',\n",
       " '_15',\n",
       " '_17',\n",
       " '_19',\n",
       " '_2',\n",
       " '_20',\n",
       " '_21',\n",
       " '_23',\n",
       " '_24',\n",
       " '_26',\n",
       " '_28',\n",
       " '_29',\n",
       " '_3',\n",
       " '_30',\n",
       " '_31',\n",
       " '_34',\n",
       " '_35',\n",
       " '_36',\n",
       " '_38',\n",
       " '_39',\n",
       " '_4',\n",
       " '_40',\n",
       " '_41',\n",
       " '_42',\n",
       " '_43',\n",
       " '_44',\n",
       " '_46',\n",
       " '_5',\n",
       " '_6',\n",
       " '_7',\n",
       " '_8',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i10',\n",
       " '_i11',\n",
       " '_i12',\n",
       " '_i13',\n",
       " '_i14',\n",
       " '_i15',\n",
       " '_i16',\n",
       " '_i17',\n",
       " '_i18',\n",
       " '_i19',\n",
       " '_i2',\n",
       " '_i20',\n",
       " '_i21',\n",
       " '_i22',\n",
       " '_i23',\n",
       " '_i24',\n",
       " '_i25',\n",
       " '_i26',\n",
       " '_i27',\n",
       " '_i28',\n",
       " '_i29',\n",
       " '_i3',\n",
       " '_i30',\n",
       " '_i31',\n",
       " '_i32',\n",
       " '_i33',\n",
       " '_i34',\n",
       " '_i35',\n",
       " '_i36',\n",
       " '_i37',\n",
       " '_i38',\n",
       " '_i39',\n",
       " '_i4',\n",
       " '_i40',\n",
       " '_i41',\n",
       " '_i42',\n",
       " '_i43',\n",
       " '_i44',\n",
       " '_i45',\n",
       " '_i46',\n",
       " '_i47',\n",
       " '_i48',\n",
       " '_i49',\n",
       " '_i5',\n",
       " '_i50',\n",
       " '_i51',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_i8',\n",
       " '_i9',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " '_pythonstartup',\n",
       " '_sh',\n",
       " 'add_files',\n",
       " 'atexit',\n",
       " 'exit',\n",
       " 'get_ipython',\n",
       " 'os',\n",
       " 'platform',\n",
       " 'py4j',\n",
       " 'pyspark',\n",
       " 'quit',\n",
       " 'rdd',\n",
       " 'rdd1',\n",
       " 'rdd2',\n",
       " 'sc',\n",
       " 'spark_home',\n",
       " 'sqlContext',\n",
       " 'sqlCtx',\n",
       " 'sys',\n",
       " 'virtual_env']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HiveContext': pyspark.sql.context.HiveContext,\n",
       " 'In': ['',\n",
       "  'sc',\n",
       "  'sc.version',\n",
       "  'sc.appName',\n",
       "  'sc.applicationId',\n",
       "  'sc.defaultParallelism',\n",
       "  'sc.defaultMinPartitions',\n",
       "  'sc.environment',\n",
       "  'sc.master',\n",
       "  'sc.show_profiles()',\n",
       "  'sc.show_profiles',\n",
       "  'sc.show_profiles()',\n",
       "  'sc.show_profiles(.)',\n",
       "  \"sc.show_profiles('.')\",\n",
       "  'sc.show_profiles()',\n",
       "  'sc.startTime',\n",
       "  'sc.sparkHome',\n",
       "  'sc.statusTracker',\n",
       "  'sc.sparkHome()',\n",
       "  'sc.statusTracker()',\n",
       "  'sc.sparkUser',\n",
       "  'sc.sparkUser()',\n",
       "  'sc.dump_profiles()',\n",
       "  'sc.pythonVer',\n",
       "  'sc.PACKAGE_EXTENSIONS',\n",
       "  'sc.pythonExec()',\n",
       "  'sc.pythonExec',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark', 'using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       "  'rdd',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd.toDebugString',\n",
       "  'rdd.toDebugString()',\n",
       "  'rdd1.collect()',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       "  'rdd1',\n",
       "  'rdd',\n",
       "  'rdd.getNumPartitions()',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'],8)\",\n",
       "  'rdd',\n",
       "  'rdd1',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd1.getNumPartitions()',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd.toDebugString()',\n",
       "  'rdd1.toDebugString()',\n",
       "  'rdd2 = rdd1.coalesce(2)',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'sc.getLocalProperty(spark.ui.enabled)',\n",
       "  'sc.getLocalProperty(sc.ui.enabled)',\n",
       "  'hostname',\n",
       "  'host',\n",
       "  'dir()',\n",
       "  'globals()'],\n",
       " 'Out': {1: <pyspark.context.SparkContext at 0x7f55e36e27b8>,\n",
       "  2: '1.6.0',\n",
       "  3: 'pyspark-shell',\n",
       "  4: 'local-1490680673806',\n",
       "  5: 16,\n",
       "  6: 2,\n",
       "  7: {'PYTHONHASHSEED': '0'},\n",
       "  8: 'local[*]',\n",
       "  10: <bound method SparkContext.show_profiles of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  15: 1490680670669,\n",
       "  17: <bound method SparkContext.statusTracker of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  19: <pyspark.status.StatusTracker at 0x7f55d14c6630>,\n",
       "  20: <bound method SparkContext.sparkUser of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  21: 'notebook',\n",
       "  23: '3.4',\n",
       "  24: ('.zip', '.egg', '.jar'),\n",
       "  26: 'python',\n",
       "  28: ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423,\n",
       "  29: 16,\n",
       "  30: <bound method RDD.toDebugString of ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423>,\n",
       "  31: b'(16) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423 []',\n",
       "  34: ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423,\n",
       "  35: ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423,\n",
       "  36: 16,\n",
       "  38: ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
       "  39: ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423,\n",
       "  40: 16,\n",
       "  41: 8,\n",
       "  42: 16,\n",
       "  43: b'(16) ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423 []',\n",
       "  44: b'(8) ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423 []',\n",
       "  46: 16,\n",
       "  51: ['HiveContext',\n",
       "   'In',\n",
       "   'Out',\n",
       "   'SQLContext',\n",
       "   'SparkContext',\n",
       "   'StorageLevel',\n",
       "   '_',\n",
       "   '_1',\n",
       "   '_10',\n",
       "   '_15',\n",
       "   '_17',\n",
       "   '_19',\n",
       "   '_2',\n",
       "   '_20',\n",
       "   '_21',\n",
       "   '_23',\n",
       "   '_24',\n",
       "   '_26',\n",
       "   '_28',\n",
       "   '_29',\n",
       "   '_3',\n",
       "   '_30',\n",
       "   '_31',\n",
       "   '_34',\n",
       "   '_35',\n",
       "   '_36',\n",
       "   '_38',\n",
       "   '_39',\n",
       "   '_4',\n",
       "   '_40',\n",
       "   '_41',\n",
       "   '_42',\n",
       "   '_43',\n",
       "   '_44',\n",
       "   '_46',\n",
       "   '_5',\n",
       "   '_6',\n",
       "   '_7',\n",
       "   '_8',\n",
       "   '__',\n",
       "   '___',\n",
       "   '__builtin__',\n",
       "   '__builtins__',\n",
       "   '__doc__',\n",
       "   '__loader__',\n",
       "   '__name__',\n",
       "   '__package__',\n",
       "   '__spec__',\n",
       "   '_dh',\n",
       "   '_i',\n",
       "   '_i1',\n",
       "   '_i10',\n",
       "   '_i11',\n",
       "   '_i12',\n",
       "   '_i13',\n",
       "   '_i14',\n",
       "   '_i15',\n",
       "   '_i16',\n",
       "   '_i17',\n",
       "   '_i18',\n",
       "   '_i19',\n",
       "   '_i2',\n",
       "   '_i20',\n",
       "   '_i21',\n",
       "   '_i22',\n",
       "   '_i23',\n",
       "   '_i24',\n",
       "   '_i25',\n",
       "   '_i26',\n",
       "   '_i27',\n",
       "   '_i28',\n",
       "   '_i29',\n",
       "   '_i3',\n",
       "   '_i30',\n",
       "   '_i31',\n",
       "   '_i32',\n",
       "   '_i33',\n",
       "   '_i34',\n",
       "   '_i35',\n",
       "   '_i36',\n",
       "   '_i37',\n",
       "   '_i38',\n",
       "   '_i39',\n",
       "   '_i4',\n",
       "   '_i40',\n",
       "   '_i41',\n",
       "   '_i42',\n",
       "   '_i43',\n",
       "   '_i44',\n",
       "   '_i45',\n",
       "   '_i46',\n",
       "   '_i47',\n",
       "   '_i48',\n",
       "   '_i49',\n",
       "   '_i5',\n",
       "   '_i50',\n",
       "   '_i51',\n",
       "   '_i6',\n",
       "   '_i7',\n",
       "   '_i8',\n",
       "   '_i9',\n",
       "   '_ih',\n",
       "   '_ii',\n",
       "   '_iii',\n",
       "   '_oh',\n",
       "   '_pythonstartup',\n",
       "   '_sh',\n",
       "   'add_files',\n",
       "   'atexit',\n",
       "   'exit',\n",
       "   'get_ipython',\n",
       "   'os',\n",
       "   'platform',\n",
       "   'py4j',\n",
       "   'pyspark',\n",
       "   'quit',\n",
       "   'rdd',\n",
       "   'rdd1',\n",
       "   'rdd2',\n",
       "   'sc',\n",
       "   'spark_home',\n",
       "   'sqlContext',\n",
       "   'sqlCtx',\n",
       "   'sys',\n",
       "   'virtual_env']},\n",
       " 'SQLContext': pyspark.sql.context.SQLContext,\n",
       " 'SparkContext': pyspark.context.SparkContext,\n",
       " 'StorageLevel': pyspark.storagelevel.StorageLevel,\n",
       " '_': ['HiveContext',\n",
       "  'In',\n",
       "  'Out',\n",
       "  'SQLContext',\n",
       "  'SparkContext',\n",
       "  'StorageLevel',\n",
       "  '_',\n",
       "  '_1',\n",
       "  '_10',\n",
       "  '_15',\n",
       "  '_17',\n",
       "  '_19',\n",
       "  '_2',\n",
       "  '_20',\n",
       "  '_21',\n",
       "  '_23',\n",
       "  '_24',\n",
       "  '_26',\n",
       "  '_28',\n",
       "  '_29',\n",
       "  '_3',\n",
       "  '_30',\n",
       "  '_31',\n",
       "  '_34',\n",
       "  '_35',\n",
       "  '_36',\n",
       "  '_38',\n",
       "  '_39',\n",
       "  '_4',\n",
       "  '_40',\n",
       "  '_41',\n",
       "  '_42',\n",
       "  '_43',\n",
       "  '_44',\n",
       "  '_46',\n",
       "  '_5',\n",
       "  '_6',\n",
       "  '_7',\n",
       "  '_8',\n",
       "  '__',\n",
       "  '___',\n",
       "  '__builtin__',\n",
       "  '__builtins__',\n",
       "  '__doc__',\n",
       "  '__loader__',\n",
       "  '__name__',\n",
       "  '__package__',\n",
       "  '__spec__',\n",
       "  '_dh',\n",
       "  '_i',\n",
       "  '_i1',\n",
       "  '_i10',\n",
       "  '_i11',\n",
       "  '_i12',\n",
       "  '_i13',\n",
       "  '_i14',\n",
       "  '_i15',\n",
       "  '_i16',\n",
       "  '_i17',\n",
       "  '_i18',\n",
       "  '_i19',\n",
       "  '_i2',\n",
       "  '_i20',\n",
       "  '_i21',\n",
       "  '_i22',\n",
       "  '_i23',\n",
       "  '_i24',\n",
       "  '_i25',\n",
       "  '_i26',\n",
       "  '_i27',\n",
       "  '_i28',\n",
       "  '_i29',\n",
       "  '_i3',\n",
       "  '_i30',\n",
       "  '_i31',\n",
       "  '_i32',\n",
       "  '_i33',\n",
       "  '_i34',\n",
       "  '_i35',\n",
       "  '_i36',\n",
       "  '_i37',\n",
       "  '_i38',\n",
       "  '_i39',\n",
       "  '_i4',\n",
       "  '_i40',\n",
       "  '_i41',\n",
       "  '_i42',\n",
       "  '_i43',\n",
       "  '_i44',\n",
       "  '_i45',\n",
       "  '_i46',\n",
       "  '_i47',\n",
       "  '_i48',\n",
       "  '_i49',\n",
       "  '_i5',\n",
       "  '_i50',\n",
       "  '_i51',\n",
       "  '_i6',\n",
       "  '_i7',\n",
       "  '_i8',\n",
       "  '_i9',\n",
       "  '_ih',\n",
       "  '_ii',\n",
       "  '_iii',\n",
       "  '_oh',\n",
       "  '_pythonstartup',\n",
       "  '_sh',\n",
       "  'add_files',\n",
       "  'atexit',\n",
       "  'exit',\n",
       "  'get_ipython',\n",
       "  'os',\n",
       "  'platform',\n",
       "  'py4j',\n",
       "  'pyspark',\n",
       "  'quit',\n",
       "  'rdd',\n",
       "  'rdd1',\n",
       "  'rdd2',\n",
       "  'sc',\n",
       "  'spark_home',\n",
       "  'sqlContext',\n",
       "  'sqlCtx',\n",
       "  'sys',\n",
       "  'virtual_env'],\n",
       " '_1': <pyspark.context.SparkContext at 0x7f55e36e27b8>,\n",
       " '_10': <bound method SparkContext.show_profiles of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       " '_15': 1490680670669,\n",
       " '_17': <bound method SparkContext.statusTracker of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       " '_19': <pyspark.status.StatusTracker at 0x7f55d14c6630>,\n",
       " '_2': '1.6.0',\n",
       " '_20': <bound method SparkContext.sparkUser of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       " '_21': 'notebook',\n",
       " '_23': '3.4',\n",
       " '_24': ('.zip', '.egg', '.jar'),\n",
       " '_26': 'python',\n",
       " '_28': ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423,\n",
       " '_29': 16,\n",
       " '_3': 'pyspark-shell',\n",
       " '_30': <bound method RDD.toDebugString of ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423>,\n",
       " '_31': b'(16) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423 []',\n",
       " '_34': ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423,\n",
       " '_35': ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423,\n",
       " '_36': 16,\n",
       " '_38': ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
       " '_39': ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423,\n",
       " '_4': 'local-1490680673806',\n",
       " '_40': 16,\n",
       " '_41': 8,\n",
       " '_42': 16,\n",
       " '_43': b'(16) ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423 []',\n",
       " '_44': b'(8) ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423 []',\n",
       " '_46': 16,\n",
       " '_5': 16,\n",
       " '_51': ['HiveContext',\n",
       "  'In',\n",
       "  'Out',\n",
       "  'SQLContext',\n",
       "  'SparkContext',\n",
       "  'StorageLevel',\n",
       "  '_',\n",
       "  '_1',\n",
       "  '_10',\n",
       "  '_15',\n",
       "  '_17',\n",
       "  '_19',\n",
       "  '_2',\n",
       "  '_20',\n",
       "  '_21',\n",
       "  '_23',\n",
       "  '_24',\n",
       "  '_26',\n",
       "  '_28',\n",
       "  '_29',\n",
       "  '_3',\n",
       "  '_30',\n",
       "  '_31',\n",
       "  '_34',\n",
       "  '_35',\n",
       "  '_36',\n",
       "  '_38',\n",
       "  '_39',\n",
       "  '_4',\n",
       "  '_40',\n",
       "  '_41',\n",
       "  '_42',\n",
       "  '_43',\n",
       "  '_44',\n",
       "  '_46',\n",
       "  '_5',\n",
       "  '_6',\n",
       "  '_7',\n",
       "  '_8',\n",
       "  '__',\n",
       "  '___',\n",
       "  '__builtin__',\n",
       "  '__builtins__',\n",
       "  '__doc__',\n",
       "  '__loader__',\n",
       "  '__name__',\n",
       "  '__package__',\n",
       "  '__spec__',\n",
       "  '_dh',\n",
       "  '_i',\n",
       "  '_i1',\n",
       "  '_i10',\n",
       "  '_i11',\n",
       "  '_i12',\n",
       "  '_i13',\n",
       "  '_i14',\n",
       "  '_i15',\n",
       "  '_i16',\n",
       "  '_i17',\n",
       "  '_i18',\n",
       "  '_i19',\n",
       "  '_i2',\n",
       "  '_i20',\n",
       "  '_i21',\n",
       "  '_i22',\n",
       "  '_i23',\n",
       "  '_i24',\n",
       "  '_i25',\n",
       "  '_i26',\n",
       "  '_i27',\n",
       "  '_i28',\n",
       "  '_i29',\n",
       "  '_i3',\n",
       "  '_i30',\n",
       "  '_i31',\n",
       "  '_i32',\n",
       "  '_i33',\n",
       "  '_i34',\n",
       "  '_i35',\n",
       "  '_i36',\n",
       "  '_i37',\n",
       "  '_i38',\n",
       "  '_i39',\n",
       "  '_i4',\n",
       "  '_i40',\n",
       "  '_i41',\n",
       "  '_i42',\n",
       "  '_i43',\n",
       "  '_i44',\n",
       "  '_i45',\n",
       "  '_i46',\n",
       "  '_i47',\n",
       "  '_i48',\n",
       "  '_i49',\n",
       "  '_i5',\n",
       "  '_i50',\n",
       "  '_i51',\n",
       "  '_i6',\n",
       "  '_i7',\n",
       "  '_i8',\n",
       "  '_i9',\n",
       "  '_ih',\n",
       "  '_ii',\n",
       "  '_iii',\n",
       "  '_oh',\n",
       "  '_pythonstartup',\n",
       "  '_sh',\n",
       "  'add_files',\n",
       "  'atexit',\n",
       "  'exit',\n",
       "  'get_ipython',\n",
       "  'os',\n",
       "  'platform',\n",
       "  'py4j',\n",
       "  'pyspark',\n",
       "  'quit',\n",
       "  'rdd',\n",
       "  'rdd1',\n",
       "  'rdd2',\n",
       "  'sc',\n",
       "  'spark_home',\n",
       "  'sqlContext',\n",
       "  'sqlCtx',\n",
       "  'sys',\n",
       "  'virtual_env'],\n",
       " '_6': 2,\n",
       " '_7': {'PYTHONHASHSEED': '0'},\n",
       " '_8': 'local[*]',\n",
       " '__': 16,\n",
       " '___': b'(8) ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423 []',\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '__doc__': '\\nAn interactive shell.\\n\\nThis file is designed to be launched as a PYTHONSTARTUP script.\\n',\n",
       " '__loader__': None,\n",
       " '__name__': '__main__',\n",
       " '__package__': None,\n",
       " '__spec__': None,\n",
       " '_dh': ['/resources'],\n",
       " '_i': 'dir()',\n",
       " '_i1': 'sc',\n",
       " '_i10': 'sc.show_profiles',\n",
       " '_i11': 'sc.show_profiles()',\n",
       " '_i12': 'sc.show_profiles(.)',\n",
       " '_i13': \"sc.show_profiles('.')\",\n",
       " '_i14': 'sc.show_profiles()',\n",
       " '_i15': 'sc.startTime',\n",
       " '_i16': 'sc.sparkHome',\n",
       " '_i17': 'sc.statusTracker',\n",
       " '_i18': 'sc.sparkHome()',\n",
       " '_i19': 'sc.statusTracker()',\n",
       " '_i2': 'sc.version',\n",
       " '_i20': 'sc.sparkUser',\n",
       " '_i21': 'sc.sparkUser()',\n",
       " '_i22': 'sc.dump_profiles()',\n",
       " '_i23': 'sc.pythonVer',\n",
       " '_i24': 'sc.PACKAGE_EXTENSIONS',\n",
       " '_i25': 'sc.pythonExec()',\n",
       " '_i26': 'sc.pythonExec',\n",
       " '_i27': \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark', 'using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       " '_i28': 'rdd',\n",
       " '_i29': 'rdd.getNumPartitions()',\n",
       " '_i3': 'sc.appName',\n",
       " '_i30': 'rdd.toDebugString',\n",
       " '_i31': 'rdd.toDebugString()',\n",
       " '_i32': 'rdd1.collect()',\n",
       " '_i33': \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       " '_i34': 'rdd1',\n",
       " '_i35': 'rdd',\n",
       " '_i36': 'rdd.getNumPartitions()',\n",
       " '_i37': \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'],8)\",\n",
       " '_i38': 'rdd',\n",
       " '_i39': 'rdd1',\n",
       " '_i4': 'sc.applicationId',\n",
       " '_i40': 'rdd.getNumPartitions()',\n",
       " '_i41': 'rdd1.getNumPartitions()',\n",
       " '_i42': 'rdd.getNumPartitions()',\n",
       " '_i43': 'rdd.toDebugString()',\n",
       " '_i44': 'rdd1.toDebugString()',\n",
       " '_i45': 'rdd2 = rdd1.coalesce(2)',\n",
       " '_i46': 'rdd.getNumPartitions()',\n",
       " '_i47': 'sc.getLocalProperty(spark.ui.enabled)',\n",
       " '_i48': 'sc.getLocalProperty(sc.ui.enabled)',\n",
       " '_i49': 'hostname',\n",
       " '_i5': 'sc.defaultParallelism',\n",
       " '_i50': 'host',\n",
       " '_i51': 'dir()',\n",
       " '_i52': 'globals()',\n",
       " '_i6': 'sc.defaultMinPartitions',\n",
       " '_i7': 'sc.environment',\n",
       " '_i8': 'sc.master',\n",
       " '_i9': 'sc.show_profiles()',\n",
       " '_ih': ['',\n",
       "  'sc',\n",
       "  'sc.version',\n",
       "  'sc.appName',\n",
       "  'sc.applicationId',\n",
       "  'sc.defaultParallelism',\n",
       "  'sc.defaultMinPartitions',\n",
       "  'sc.environment',\n",
       "  'sc.master',\n",
       "  'sc.show_profiles()',\n",
       "  'sc.show_profiles',\n",
       "  'sc.show_profiles()',\n",
       "  'sc.show_profiles(.)',\n",
       "  \"sc.show_profiles('.')\",\n",
       "  'sc.show_profiles()',\n",
       "  'sc.startTime',\n",
       "  'sc.sparkHome',\n",
       "  'sc.statusTracker',\n",
       "  'sc.sparkHome()',\n",
       "  'sc.statusTracker()',\n",
       "  'sc.sparkUser',\n",
       "  'sc.sparkUser()',\n",
       "  'sc.dump_profiles()',\n",
       "  'sc.pythonVer',\n",
       "  'sc.PACKAGE_EXTENSIONS',\n",
       "  'sc.pythonExec()',\n",
       "  'sc.pythonExec',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark', 'using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       "  'rdd',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd.toDebugString',\n",
       "  'rdd.toDebugString()',\n",
       "  'rdd1.collect()',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       "  'rdd1',\n",
       "  'rdd',\n",
       "  'rdd.getNumPartitions()',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'],8)\",\n",
       "  'rdd',\n",
       "  'rdd1',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd1.getNumPartitions()',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd.toDebugString()',\n",
       "  'rdd1.toDebugString()',\n",
       "  'rdd2 = rdd1.coalesce(2)',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'sc.getLocalProperty(spark.ui.enabled)',\n",
       "  'sc.getLocalProperty(sc.ui.enabled)',\n",
       "  'hostname',\n",
       "  'host',\n",
       "  'dir()',\n",
       "  'globals()'],\n",
       " '_ii': 'host',\n",
       " '_iii': 'hostname',\n",
       " '_oh': {1: <pyspark.context.SparkContext at 0x7f55e36e27b8>,\n",
       "  2: '1.6.0',\n",
       "  3: 'pyspark-shell',\n",
       "  4: 'local-1490680673806',\n",
       "  5: 16,\n",
       "  6: 2,\n",
       "  7: {'PYTHONHASHSEED': '0'},\n",
       "  8: 'local[*]',\n",
       "  10: <bound method SparkContext.show_profiles of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  15: 1490680670669,\n",
       "  17: <bound method SparkContext.statusTracker of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  19: <pyspark.status.StatusTracker at 0x7f55d14c6630>,\n",
       "  20: <bound method SparkContext.sparkUser of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  21: 'notebook',\n",
       "  23: '3.4',\n",
       "  24: ('.zip', '.egg', '.jar'),\n",
       "  26: 'python',\n",
       "  28: ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423,\n",
       "  29: 16,\n",
       "  30: <bound method RDD.toDebugString of ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423>,\n",
       "  31: b'(16) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423 []',\n",
       "  34: ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423,\n",
       "  35: ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423,\n",
       "  36: 16,\n",
       "  38: ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
       "  39: ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423,\n",
       "  40: 16,\n",
       "  41: 8,\n",
       "  42: 16,\n",
       "  43: b'(16) ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423 []',\n",
       "  44: b'(8) ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423 []',\n",
       "  46: 16,\n",
       "  51: ['HiveContext',\n",
       "   'In',\n",
       "   'Out',\n",
       "   'SQLContext',\n",
       "   'SparkContext',\n",
       "   'StorageLevel',\n",
       "   '_',\n",
       "   '_1',\n",
       "   '_10',\n",
       "   '_15',\n",
       "   '_17',\n",
       "   '_19',\n",
       "   '_2',\n",
       "   '_20',\n",
       "   '_21',\n",
       "   '_23',\n",
       "   '_24',\n",
       "   '_26',\n",
       "   '_28',\n",
       "   '_29',\n",
       "   '_3',\n",
       "   '_30',\n",
       "   '_31',\n",
       "   '_34',\n",
       "   '_35',\n",
       "   '_36',\n",
       "   '_38',\n",
       "   '_39',\n",
       "   '_4',\n",
       "   '_40',\n",
       "   '_41',\n",
       "   '_42',\n",
       "   '_43',\n",
       "   '_44',\n",
       "   '_46',\n",
       "   '_5',\n",
       "   '_6',\n",
       "   '_7',\n",
       "   '_8',\n",
       "   '__',\n",
       "   '___',\n",
       "   '__builtin__',\n",
       "   '__builtins__',\n",
       "   '__doc__',\n",
       "   '__loader__',\n",
       "   '__name__',\n",
       "   '__package__',\n",
       "   '__spec__',\n",
       "   '_dh',\n",
       "   '_i',\n",
       "   '_i1',\n",
       "   '_i10',\n",
       "   '_i11',\n",
       "   '_i12',\n",
       "   '_i13',\n",
       "   '_i14',\n",
       "   '_i15',\n",
       "   '_i16',\n",
       "   '_i17',\n",
       "   '_i18',\n",
       "   '_i19',\n",
       "   '_i2',\n",
       "   '_i20',\n",
       "   '_i21',\n",
       "   '_i22',\n",
       "   '_i23',\n",
       "   '_i24',\n",
       "   '_i25',\n",
       "   '_i26',\n",
       "   '_i27',\n",
       "   '_i28',\n",
       "   '_i29',\n",
       "   '_i3',\n",
       "   '_i30',\n",
       "   '_i31',\n",
       "   '_i32',\n",
       "   '_i33',\n",
       "   '_i34',\n",
       "   '_i35',\n",
       "   '_i36',\n",
       "   '_i37',\n",
       "   '_i38',\n",
       "   '_i39',\n",
       "   '_i4',\n",
       "   '_i40',\n",
       "   '_i41',\n",
       "   '_i42',\n",
       "   '_i43',\n",
       "   '_i44',\n",
       "   '_i45',\n",
       "   '_i46',\n",
       "   '_i47',\n",
       "   '_i48',\n",
       "   '_i49',\n",
       "   '_i5',\n",
       "   '_i50',\n",
       "   '_i51',\n",
       "   '_i6',\n",
       "   '_i7',\n",
       "   '_i8',\n",
       "   '_i9',\n",
       "   '_ih',\n",
       "   '_ii',\n",
       "   '_iii',\n",
       "   '_oh',\n",
       "   '_pythonstartup',\n",
       "   '_sh',\n",
       "   'add_files',\n",
       "   'atexit',\n",
       "   'exit',\n",
       "   'get_ipython',\n",
       "   'os',\n",
       "   'platform',\n",
       "   'py4j',\n",
       "   'pyspark',\n",
       "   'quit',\n",
       "   'rdd',\n",
       "   'rdd1',\n",
       "   'rdd2',\n",
       "   'sc',\n",
       "   'spark_home',\n",
       "   'sqlContext',\n",
       "   'sqlCtx',\n",
       "   'sys',\n",
       "   'virtual_env']},\n",
       " '_pythonstartup': None,\n",
       " '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python3.4/dist-packages/IPython/core/shadowns.py'>,\n",
       " 'add_files': None,\n",
       " 'atexit': <module 'atexit' (built-in)>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f55e37b4358>,\n",
       " 'get_ipython': <bound method ZMQInteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f55f28a37f0>>,\n",
       " 'os': <module 'os' from '/resources/common/.virtualenv/python3/lib/python3.4/os.py'>,\n",
       " 'platform': <module 'platform' from '/usr/lib/python3.4/platform.py'>,\n",
       " 'py4j': <module 'py4j' from '/home/notebook/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/__init__.py'>,\n",
       " 'pyspark': <module 'pyspark' from '/home/notebook/spark-1.6.0-bin-hadoop2.6/python/pyspark/__init__.py'>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f55e37b4358>,\n",
       " 'rdd': ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
       " 'rdd1': ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423,\n",
       " 'rdd2': CoalescedRDD[5] at coalesce at NativeMethodAccessorImpl.java:-2,\n",
       " 'sc': <pyspark.context.SparkContext at 0x7f55e36e27b8>,\n",
       " 'spark_home': '/home/notebook/spark-1.6.0-bin-hadoop2.6',\n",
       " 'sqlContext': <pyspark.sql.context.HiveContext at 0x7f55d27049b0>,\n",
       " 'sqlCtx': <pyspark.sql.context.HiveContext at 0x7f55d27049b0>,\n",
       " 'sys': <module 'sys' (built-in)>,\n",
       " 'virtual_env': '/resources/common/.virtualenv/python3'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HiveContext': pyspark.sql.context.HiveContext,\n",
       " 'In': ['',\n",
       "  'sc',\n",
       "  'sc.version',\n",
       "  'sc.appName',\n",
       "  'sc.applicationId',\n",
       "  'sc.defaultParallelism',\n",
       "  'sc.defaultMinPartitions',\n",
       "  'sc.environment',\n",
       "  'sc.master',\n",
       "  'sc.show_profiles()',\n",
       "  'sc.show_profiles',\n",
       "  'sc.show_profiles()',\n",
       "  'sc.show_profiles(.)',\n",
       "  \"sc.show_profiles('.')\",\n",
       "  'sc.show_profiles()',\n",
       "  'sc.startTime',\n",
       "  'sc.sparkHome',\n",
       "  'sc.statusTracker',\n",
       "  'sc.sparkHome()',\n",
       "  'sc.statusTracker()',\n",
       "  'sc.sparkUser',\n",
       "  'sc.sparkUser()',\n",
       "  'sc.dump_profiles()',\n",
       "  'sc.pythonVer',\n",
       "  'sc.PACKAGE_EXTENSIONS',\n",
       "  'sc.pythonExec()',\n",
       "  'sc.pythonExec',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark', 'using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       "  'rdd',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd.toDebugString',\n",
       "  'rdd.toDebugString()',\n",
       "  'rdd1.collect()',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       "  'rdd1',\n",
       "  'rdd',\n",
       "  'rdd.getNumPartitions()',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'],8)\",\n",
       "  'rdd',\n",
       "  'rdd1',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd1.getNumPartitions()',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd.toDebugString()',\n",
       "  'rdd1.toDebugString()',\n",
       "  'rdd2 = rdd1.coalesce(2)',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'sc.getLocalProperty(spark.ui.enabled)',\n",
       "  'sc.getLocalProperty(sc.ui.enabled)',\n",
       "  'hostname',\n",
       "  'host',\n",
       "  'dir()',\n",
       "  'globals()',\n",
       "  'locals()'],\n",
       " 'Out': {1: <pyspark.context.SparkContext at 0x7f55e36e27b8>,\n",
       "  2: '1.6.0',\n",
       "  3: 'pyspark-shell',\n",
       "  4: 'local-1490680673806',\n",
       "  5: 16,\n",
       "  6: 2,\n",
       "  7: {'PYTHONHASHSEED': '0'},\n",
       "  8: 'local[*]',\n",
       "  10: <bound method SparkContext.show_profiles of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  15: 1490680670669,\n",
       "  17: <bound method SparkContext.statusTracker of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  19: <pyspark.status.StatusTracker at 0x7f55d14c6630>,\n",
       "  20: <bound method SparkContext.sparkUser of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  21: 'notebook',\n",
       "  23: '3.4',\n",
       "  24: ('.zip', '.egg', '.jar'),\n",
       "  26: 'python',\n",
       "  28: ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423,\n",
       "  29: 16,\n",
       "  30: <bound method RDD.toDebugString of ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423>,\n",
       "  31: b'(16) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423 []',\n",
       "  34: ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423,\n",
       "  35: ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423,\n",
       "  36: 16,\n",
       "  38: ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
       "  39: ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423,\n",
       "  40: 16,\n",
       "  41: 8,\n",
       "  42: 16,\n",
       "  43: b'(16) ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423 []',\n",
       "  44: b'(8) ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423 []',\n",
       "  46: 16,\n",
       "  51: ['HiveContext',\n",
       "   'In',\n",
       "   'Out',\n",
       "   'SQLContext',\n",
       "   'SparkContext',\n",
       "   'StorageLevel',\n",
       "   '_',\n",
       "   '_1',\n",
       "   '_10',\n",
       "   '_15',\n",
       "   '_17',\n",
       "   '_19',\n",
       "   '_2',\n",
       "   '_20',\n",
       "   '_21',\n",
       "   '_23',\n",
       "   '_24',\n",
       "   '_26',\n",
       "   '_28',\n",
       "   '_29',\n",
       "   '_3',\n",
       "   '_30',\n",
       "   '_31',\n",
       "   '_34',\n",
       "   '_35',\n",
       "   '_36',\n",
       "   '_38',\n",
       "   '_39',\n",
       "   '_4',\n",
       "   '_40',\n",
       "   '_41',\n",
       "   '_42',\n",
       "   '_43',\n",
       "   '_44',\n",
       "   '_46',\n",
       "   '_5',\n",
       "   '_6',\n",
       "   '_7',\n",
       "   '_8',\n",
       "   '__',\n",
       "   '___',\n",
       "   '__builtin__',\n",
       "   '__builtins__',\n",
       "   '__doc__',\n",
       "   '__loader__',\n",
       "   '__name__',\n",
       "   '__package__',\n",
       "   '__spec__',\n",
       "   '_dh',\n",
       "   '_i',\n",
       "   '_i1',\n",
       "   '_i10',\n",
       "   '_i11',\n",
       "   '_i12',\n",
       "   '_i13',\n",
       "   '_i14',\n",
       "   '_i15',\n",
       "   '_i16',\n",
       "   '_i17',\n",
       "   '_i18',\n",
       "   '_i19',\n",
       "   '_i2',\n",
       "   '_i20',\n",
       "   '_i21',\n",
       "   '_i22',\n",
       "   '_i23',\n",
       "   '_i24',\n",
       "   '_i25',\n",
       "   '_i26',\n",
       "   '_i27',\n",
       "   '_i28',\n",
       "   '_i29',\n",
       "   '_i3',\n",
       "   '_i30',\n",
       "   '_i31',\n",
       "   '_i32',\n",
       "   '_i33',\n",
       "   '_i34',\n",
       "   '_i35',\n",
       "   '_i36',\n",
       "   '_i37',\n",
       "   '_i38',\n",
       "   '_i39',\n",
       "   '_i4',\n",
       "   '_i40',\n",
       "   '_i41',\n",
       "   '_i42',\n",
       "   '_i43',\n",
       "   '_i44',\n",
       "   '_i45',\n",
       "   '_i46',\n",
       "   '_i47',\n",
       "   '_i48',\n",
       "   '_i49',\n",
       "   '_i5',\n",
       "   '_i50',\n",
       "   '_i51',\n",
       "   '_i6',\n",
       "   '_i7',\n",
       "   '_i8',\n",
       "   '_i9',\n",
       "   '_ih',\n",
       "   '_ii',\n",
       "   '_iii',\n",
       "   '_oh',\n",
       "   '_pythonstartup',\n",
       "   '_sh',\n",
       "   'add_files',\n",
       "   'atexit',\n",
       "   'exit',\n",
       "   'get_ipython',\n",
       "   'os',\n",
       "   'platform',\n",
       "   'py4j',\n",
       "   'pyspark',\n",
       "   'quit',\n",
       "   'rdd',\n",
       "   'rdd1',\n",
       "   'rdd2',\n",
       "   'sc',\n",
       "   'spark_home',\n",
       "   'sqlContext',\n",
       "   'sqlCtx',\n",
       "   'sys',\n",
       "   'virtual_env'],\n",
       "  52: {...}},\n",
       " 'SQLContext': pyspark.sql.context.SQLContext,\n",
       " 'SparkContext': pyspark.context.SparkContext,\n",
       " 'StorageLevel': pyspark.storagelevel.StorageLevel,\n",
       " '_': {...},\n",
       " '_1': <pyspark.context.SparkContext at 0x7f55e36e27b8>,\n",
       " '_10': <bound method SparkContext.show_profiles of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       " '_15': 1490680670669,\n",
       " '_17': <bound method SparkContext.statusTracker of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       " '_19': <pyspark.status.StatusTracker at 0x7f55d14c6630>,\n",
       " '_2': '1.6.0',\n",
       " '_20': <bound method SparkContext.sparkUser of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       " '_21': 'notebook',\n",
       " '_23': '3.4',\n",
       " '_24': ('.zip', '.egg', '.jar'),\n",
       " '_26': 'python',\n",
       " '_28': ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423,\n",
       " '_29': 16,\n",
       " '_3': 'pyspark-shell',\n",
       " '_30': <bound method RDD.toDebugString of ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423>,\n",
       " '_31': b'(16) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423 []',\n",
       " '_34': ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423,\n",
       " '_35': ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423,\n",
       " '_36': 16,\n",
       " '_38': ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
       " '_39': ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423,\n",
       " '_4': 'local-1490680673806',\n",
       " '_40': 16,\n",
       " '_41': 8,\n",
       " '_42': 16,\n",
       " '_43': b'(16) ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423 []',\n",
       " '_44': b'(8) ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423 []',\n",
       " '_46': 16,\n",
       " '_5': 16,\n",
       " '_51': ['HiveContext',\n",
       "  'In',\n",
       "  'Out',\n",
       "  'SQLContext',\n",
       "  'SparkContext',\n",
       "  'StorageLevel',\n",
       "  '_',\n",
       "  '_1',\n",
       "  '_10',\n",
       "  '_15',\n",
       "  '_17',\n",
       "  '_19',\n",
       "  '_2',\n",
       "  '_20',\n",
       "  '_21',\n",
       "  '_23',\n",
       "  '_24',\n",
       "  '_26',\n",
       "  '_28',\n",
       "  '_29',\n",
       "  '_3',\n",
       "  '_30',\n",
       "  '_31',\n",
       "  '_34',\n",
       "  '_35',\n",
       "  '_36',\n",
       "  '_38',\n",
       "  '_39',\n",
       "  '_4',\n",
       "  '_40',\n",
       "  '_41',\n",
       "  '_42',\n",
       "  '_43',\n",
       "  '_44',\n",
       "  '_46',\n",
       "  '_5',\n",
       "  '_6',\n",
       "  '_7',\n",
       "  '_8',\n",
       "  '__',\n",
       "  '___',\n",
       "  '__builtin__',\n",
       "  '__builtins__',\n",
       "  '__doc__',\n",
       "  '__loader__',\n",
       "  '__name__',\n",
       "  '__package__',\n",
       "  '__spec__',\n",
       "  '_dh',\n",
       "  '_i',\n",
       "  '_i1',\n",
       "  '_i10',\n",
       "  '_i11',\n",
       "  '_i12',\n",
       "  '_i13',\n",
       "  '_i14',\n",
       "  '_i15',\n",
       "  '_i16',\n",
       "  '_i17',\n",
       "  '_i18',\n",
       "  '_i19',\n",
       "  '_i2',\n",
       "  '_i20',\n",
       "  '_i21',\n",
       "  '_i22',\n",
       "  '_i23',\n",
       "  '_i24',\n",
       "  '_i25',\n",
       "  '_i26',\n",
       "  '_i27',\n",
       "  '_i28',\n",
       "  '_i29',\n",
       "  '_i3',\n",
       "  '_i30',\n",
       "  '_i31',\n",
       "  '_i32',\n",
       "  '_i33',\n",
       "  '_i34',\n",
       "  '_i35',\n",
       "  '_i36',\n",
       "  '_i37',\n",
       "  '_i38',\n",
       "  '_i39',\n",
       "  '_i4',\n",
       "  '_i40',\n",
       "  '_i41',\n",
       "  '_i42',\n",
       "  '_i43',\n",
       "  '_i44',\n",
       "  '_i45',\n",
       "  '_i46',\n",
       "  '_i47',\n",
       "  '_i48',\n",
       "  '_i49',\n",
       "  '_i5',\n",
       "  '_i50',\n",
       "  '_i51',\n",
       "  '_i6',\n",
       "  '_i7',\n",
       "  '_i8',\n",
       "  '_i9',\n",
       "  '_ih',\n",
       "  '_ii',\n",
       "  '_iii',\n",
       "  '_oh',\n",
       "  '_pythonstartup',\n",
       "  '_sh',\n",
       "  'add_files',\n",
       "  'atexit',\n",
       "  'exit',\n",
       "  'get_ipython',\n",
       "  'os',\n",
       "  'platform',\n",
       "  'py4j',\n",
       "  'pyspark',\n",
       "  'quit',\n",
       "  'rdd',\n",
       "  'rdd1',\n",
       "  'rdd2',\n",
       "  'sc',\n",
       "  'spark_home',\n",
       "  'sqlContext',\n",
       "  'sqlCtx',\n",
       "  'sys',\n",
       "  'virtual_env'],\n",
       " '_52': {...},\n",
       " '_6': 2,\n",
       " '_7': {'PYTHONHASHSEED': '0'},\n",
       " '_8': 'local[*]',\n",
       " '__': ['HiveContext',\n",
       "  'In',\n",
       "  'Out',\n",
       "  'SQLContext',\n",
       "  'SparkContext',\n",
       "  'StorageLevel',\n",
       "  '_',\n",
       "  '_1',\n",
       "  '_10',\n",
       "  '_15',\n",
       "  '_17',\n",
       "  '_19',\n",
       "  '_2',\n",
       "  '_20',\n",
       "  '_21',\n",
       "  '_23',\n",
       "  '_24',\n",
       "  '_26',\n",
       "  '_28',\n",
       "  '_29',\n",
       "  '_3',\n",
       "  '_30',\n",
       "  '_31',\n",
       "  '_34',\n",
       "  '_35',\n",
       "  '_36',\n",
       "  '_38',\n",
       "  '_39',\n",
       "  '_4',\n",
       "  '_40',\n",
       "  '_41',\n",
       "  '_42',\n",
       "  '_43',\n",
       "  '_44',\n",
       "  '_46',\n",
       "  '_5',\n",
       "  '_6',\n",
       "  '_7',\n",
       "  '_8',\n",
       "  '__',\n",
       "  '___',\n",
       "  '__builtin__',\n",
       "  '__builtins__',\n",
       "  '__doc__',\n",
       "  '__loader__',\n",
       "  '__name__',\n",
       "  '__package__',\n",
       "  '__spec__',\n",
       "  '_dh',\n",
       "  '_i',\n",
       "  '_i1',\n",
       "  '_i10',\n",
       "  '_i11',\n",
       "  '_i12',\n",
       "  '_i13',\n",
       "  '_i14',\n",
       "  '_i15',\n",
       "  '_i16',\n",
       "  '_i17',\n",
       "  '_i18',\n",
       "  '_i19',\n",
       "  '_i2',\n",
       "  '_i20',\n",
       "  '_i21',\n",
       "  '_i22',\n",
       "  '_i23',\n",
       "  '_i24',\n",
       "  '_i25',\n",
       "  '_i26',\n",
       "  '_i27',\n",
       "  '_i28',\n",
       "  '_i29',\n",
       "  '_i3',\n",
       "  '_i30',\n",
       "  '_i31',\n",
       "  '_i32',\n",
       "  '_i33',\n",
       "  '_i34',\n",
       "  '_i35',\n",
       "  '_i36',\n",
       "  '_i37',\n",
       "  '_i38',\n",
       "  '_i39',\n",
       "  '_i4',\n",
       "  '_i40',\n",
       "  '_i41',\n",
       "  '_i42',\n",
       "  '_i43',\n",
       "  '_i44',\n",
       "  '_i45',\n",
       "  '_i46',\n",
       "  '_i47',\n",
       "  '_i48',\n",
       "  '_i49',\n",
       "  '_i5',\n",
       "  '_i50',\n",
       "  '_i51',\n",
       "  '_i6',\n",
       "  '_i7',\n",
       "  '_i8',\n",
       "  '_i9',\n",
       "  '_ih',\n",
       "  '_ii',\n",
       "  '_iii',\n",
       "  '_oh',\n",
       "  '_pythonstartup',\n",
       "  '_sh',\n",
       "  'add_files',\n",
       "  'atexit',\n",
       "  'exit',\n",
       "  'get_ipython',\n",
       "  'os',\n",
       "  'platform',\n",
       "  'py4j',\n",
       "  'pyspark',\n",
       "  'quit',\n",
       "  'rdd',\n",
       "  'rdd1',\n",
       "  'rdd2',\n",
       "  'sc',\n",
       "  'spark_home',\n",
       "  'sqlContext',\n",
       "  'sqlCtx',\n",
       "  'sys',\n",
       "  'virtual_env'],\n",
       " '___': 16,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '__doc__': '\\nAn interactive shell.\\n\\nThis file is designed to be launched as a PYTHONSTARTUP script.\\n',\n",
       " '__loader__': None,\n",
       " '__name__': '__main__',\n",
       " '__package__': None,\n",
       " '__spec__': None,\n",
       " '_dh': ['/resources'],\n",
       " '_i': 'globals()',\n",
       " '_i1': 'sc',\n",
       " '_i10': 'sc.show_profiles',\n",
       " '_i11': 'sc.show_profiles()',\n",
       " '_i12': 'sc.show_profiles(.)',\n",
       " '_i13': \"sc.show_profiles('.')\",\n",
       " '_i14': 'sc.show_profiles()',\n",
       " '_i15': 'sc.startTime',\n",
       " '_i16': 'sc.sparkHome',\n",
       " '_i17': 'sc.statusTracker',\n",
       " '_i18': 'sc.sparkHome()',\n",
       " '_i19': 'sc.statusTracker()',\n",
       " '_i2': 'sc.version',\n",
       " '_i20': 'sc.sparkUser',\n",
       " '_i21': 'sc.sparkUser()',\n",
       " '_i22': 'sc.dump_profiles()',\n",
       " '_i23': 'sc.pythonVer',\n",
       " '_i24': 'sc.PACKAGE_EXTENSIONS',\n",
       " '_i25': 'sc.pythonExec()',\n",
       " '_i26': 'sc.pythonExec',\n",
       " '_i27': \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark', 'using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       " '_i28': 'rdd',\n",
       " '_i29': 'rdd.getNumPartitions()',\n",
       " '_i3': 'sc.appName',\n",
       " '_i30': 'rdd.toDebugString',\n",
       " '_i31': 'rdd.toDebugString()',\n",
       " '_i32': 'rdd1.collect()',\n",
       " '_i33': \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       " '_i34': 'rdd1',\n",
       " '_i35': 'rdd',\n",
       " '_i36': 'rdd.getNumPartitions()',\n",
       " '_i37': \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'],8)\",\n",
       " '_i38': 'rdd',\n",
       " '_i39': 'rdd1',\n",
       " '_i4': 'sc.applicationId',\n",
       " '_i40': 'rdd.getNumPartitions()',\n",
       " '_i41': 'rdd1.getNumPartitions()',\n",
       " '_i42': 'rdd.getNumPartitions()',\n",
       " '_i43': 'rdd.toDebugString()',\n",
       " '_i44': 'rdd1.toDebugString()',\n",
       " '_i45': 'rdd2 = rdd1.coalesce(2)',\n",
       " '_i46': 'rdd.getNumPartitions()',\n",
       " '_i47': 'sc.getLocalProperty(spark.ui.enabled)',\n",
       " '_i48': 'sc.getLocalProperty(sc.ui.enabled)',\n",
       " '_i49': 'hostname',\n",
       " '_i5': 'sc.defaultParallelism',\n",
       " '_i50': 'host',\n",
       " '_i51': 'dir()',\n",
       " '_i52': 'globals()',\n",
       " '_i53': 'locals()',\n",
       " '_i6': 'sc.defaultMinPartitions',\n",
       " '_i7': 'sc.environment',\n",
       " '_i8': 'sc.master',\n",
       " '_i9': 'sc.show_profiles()',\n",
       " '_ih': ['',\n",
       "  'sc',\n",
       "  'sc.version',\n",
       "  'sc.appName',\n",
       "  'sc.applicationId',\n",
       "  'sc.defaultParallelism',\n",
       "  'sc.defaultMinPartitions',\n",
       "  'sc.environment',\n",
       "  'sc.master',\n",
       "  'sc.show_profiles()',\n",
       "  'sc.show_profiles',\n",
       "  'sc.show_profiles()',\n",
       "  'sc.show_profiles(.)',\n",
       "  \"sc.show_profiles('.')\",\n",
       "  'sc.show_profiles()',\n",
       "  'sc.startTime',\n",
       "  'sc.sparkHome',\n",
       "  'sc.statusTracker',\n",
       "  'sc.sparkHome()',\n",
       "  'sc.statusTracker()',\n",
       "  'sc.sparkUser',\n",
       "  'sc.sparkUser()',\n",
       "  'sc.dump_profiles()',\n",
       "  'sc.pythonVer',\n",
       "  'sc.PACKAGE_EXTENSIONS',\n",
       "  'sc.pythonExec()',\n",
       "  'sc.pythonExec',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark', 'using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       "  'rdd',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd.toDebugString',\n",
       "  'rdd.toDebugString()',\n",
       "  'rdd1.collect()',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'])\",\n",
       "  'rdd1',\n",
       "  'rdd',\n",
       "  'rdd.getNumPartitions()',\n",
       "  \"rdd = sc.parallelize(['I', 'will', 'complete', 'learning', 'Spark'])\\nrdd1 = sc.parallelize(['using', 'Jupyter' , 'Notebook', 'in', '20', 'days'],8)\",\n",
       "  'rdd',\n",
       "  'rdd1',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd1.getNumPartitions()',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'rdd.toDebugString()',\n",
       "  'rdd1.toDebugString()',\n",
       "  'rdd2 = rdd1.coalesce(2)',\n",
       "  'rdd.getNumPartitions()',\n",
       "  'sc.getLocalProperty(spark.ui.enabled)',\n",
       "  'sc.getLocalProperty(sc.ui.enabled)',\n",
       "  'hostname',\n",
       "  'host',\n",
       "  'dir()',\n",
       "  'globals()',\n",
       "  'locals()'],\n",
       " '_ii': 'dir()',\n",
       " '_iii': 'host',\n",
       " '_oh': {1: <pyspark.context.SparkContext at 0x7f55e36e27b8>,\n",
       "  2: '1.6.0',\n",
       "  3: 'pyspark-shell',\n",
       "  4: 'local-1490680673806',\n",
       "  5: 16,\n",
       "  6: 2,\n",
       "  7: {'PYTHONHASHSEED': '0'},\n",
       "  8: 'local[*]',\n",
       "  10: <bound method SparkContext.show_profiles of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  15: 1490680670669,\n",
       "  17: <bound method SparkContext.statusTracker of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  19: <pyspark.status.StatusTracker at 0x7f55d14c6630>,\n",
       "  20: <bound method SparkContext.sparkUser of <pyspark.context.SparkContext object at 0x7f55e36e27b8>>,\n",
       "  21: 'notebook',\n",
       "  23: '3.4',\n",
       "  24: ('.zip', '.egg', '.jar'),\n",
       "  26: 'python',\n",
       "  28: ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423,\n",
       "  29: 16,\n",
       "  30: <bound method RDD.toDebugString of ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423>,\n",
       "  31: b'(16) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423 []',\n",
       "  34: ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423,\n",
       "  35: ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423,\n",
       "  36: 16,\n",
       "  38: ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
       "  39: ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423,\n",
       "  40: 16,\n",
       "  41: 8,\n",
       "  42: 16,\n",
       "  43: b'(16) ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423 []',\n",
       "  44: b'(8) ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423 []',\n",
       "  46: 16,\n",
       "  51: ['HiveContext',\n",
       "   'In',\n",
       "   'Out',\n",
       "   'SQLContext',\n",
       "   'SparkContext',\n",
       "   'StorageLevel',\n",
       "   '_',\n",
       "   '_1',\n",
       "   '_10',\n",
       "   '_15',\n",
       "   '_17',\n",
       "   '_19',\n",
       "   '_2',\n",
       "   '_20',\n",
       "   '_21',\n",
       "   '_23',\n",
       "   '_24',\n",
       "   '_26',\n",
       "   '_28',\n",
       "   '_29',\n",
       "   '_3',\n",
       "   '_30',\n",
       "   '_31',\n",
       "   '_34',\n",
       "   '_35',\n",
       "   '_36',\n",
       "   '_38',\n",
       "   '_39',\n",
       "   '_4',\n",
       "   '_40',\n",
       "   '_41',\n",
       "   '_42',\n",
       "   '_43',\n",
       "   '_44',\n",
       "   '_46',\n",
       "   '_5',\n",
       "   '_6',\n",
       "   '_7',\n",
       "   '_8',\n",
       "   '__',\n",
       "   '___',\n",
       "   '__builtin__',\n",
       "   '__builtins__',\n",
       "   '__doc__',\n",
       "   '__loader__',\n",
       "   '__name__',\n",
       "   '__package__',\n",
       "   '__spec__',\n",
       "   '_dh',\n",
       "   '_i',\n",
       "   '_i1',\n",
       "   '_i10',\n",
       "   '_i11',\n",
       "   '_i12',\n",
       "   '_i13',\n",
       "   '_i14',\n",
       "   '_i15',\n",
       "   '_i16',\n",
       "   '_i17',\n",
       "   '_i18',\n",
       "   '_i19',\n",
       "   '_i2',\n",
       "   '_i20',\n",
       "   '_i21',\n",
       "   '_i22',\n",
       "   '_i23',\n",
       "   '_i24',\n",
       "   '_i25',\n",
       "   '_i26',\n",
       "   '_i27',\n",
       "   '_i28',\n",
       "   '_i29',\n",
       "   '_i3',\n",
       "   '_i30',\n",
       "   '_i31',\n",
       "   '_i32',\n",
       "   '_i33',\n",
       "   '_i34',\n",
       "   '_i35',\n",
       "   '_i36',\n",
       "   '_i37',\n",
       "   '_i38',\n",
       "   '_i39',\n",
       "   '_i4',\n",
       "   '_i40',\n",
       "   '_i41',\n",
       "   '_i42',\n",
       "   '_i43',\n",
       "   '_i44',\n",
       "   '_i45',\n",
       "   '_i46',\n",
       "   '_i47',\n",
       "   '_i48',\n",
       "   '_i49',\n",
       "   '_i5',\n",
       "   '_i50',\n",
       "   '_i51',\n",
       "   '_i6',\n",
       "   '_i7',\n",
       "   '_i8',\n",
       "   '_i9',\n",
       "   '_ih',\n",
       "   '_ii',\n",
       "   '_iii',\n",
       "   '_oh',\n",
       "   '_pythonstartup',\n",
       "   '_sh',\n",
       "   'add_files',\n",
       "   'atexit',\n",
       "   'exit',\n",
       "   'get_ipython',\n",
       "   'os',\n",
       "   'platform',\n",
       "   'py4j',\n",
       "   'pyspark',\n",
       "   'quit',\n",
       "   'rdd',\n",
       "   'rdd1',\n",
       "   'rdd2',\n",
       "   'sc',\n",
       "   'spark_home',\n",
       "   'sqlContext',\n",
       "   'sqlCtx',\n",
       "   'sys',\n",
       "   'virtual_env'],\n",
       "  52: {...}},\n",
       " '_pythonstartup': None,\n",
       " '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python3.4/dist-packages/IPython/core/shadowns.py'>,\n",
       " 'add_files': None,\n",
       " 'atexit': <module 'atexit' (built-in)>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f55e37b4358>,\n",
       " 'get_ipython': <bound method ZMQInteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f55f28a37f0>>,\n",
       " 'os': <module 'os' from '/resources/common/.virtualenv/python3/lib/python3.4/os.py'>,\n",
       " 'platform': <module 'platform' from '/usr/lib/python3.4/platform.py'>,\n",
       " 'py4j': <module 'py4j' from '/home/notebook/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/__init__.py'>,\n",
       " 'pyspark': <module 'pyspark' from '/home/notebook/spark-1.6.0-bin-hadoop2.6/python/pyspark/__init__.py'>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f55e37b4358>,\n",
       " 'rdd': ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
       " 'rdd1': ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423,\n",
       " 'rdd2': CoalescedRDD[5] at coalesce at NativeMethodAccessorImpl.java:-2,\n",
       " 'sc': <pyspark.context.SparkContext at 0x7f55e36e27b8>,\n",
       " 'spark_home': '/home/notebook/spark-1.6.0-bin-hadoop2.6',\n",
       " 'sqlContext': <pyspark.sql.context.HiveContext at 0x7f55d27049b0>,\n",
       " 'sqlCtx': <pyspark.sql.context.HiveContext at 0x7f55d27049b0>,\n",
       " 'sys': <module 'sys' (built-in)>,\n",
       " 'virtual_env': '/resources/common/.virtualenv/python3'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "a = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraClassPath',\n",
       "  '/ngwb/jars/avro-1.7.6.jar:/ngwb/jars/oms_client-11.5.1.0.200.jar:/ngwb/jars/commons-csv-1.1.jar:/ngwb/jars/oms_common-11.5.1.0.200.jar:/ngwb/jars/genericds_2.10-1.0.jar:/ngwb/jars/simple-regex-4.6.jar:/ngwb/jars/htmllexer-2.1.jar:/ngwb/jars/spark-avro_2.10-2.0.1.jar:/ngwb/jars/htmlparser-2.1.jar:/ngwb/jars/spark-csv_2.10-1.3.0.jar:/ngwb/jars/ibmprofile_2.10-1.0.jar:/ngwb/jars/system-t-runtime-4.6-biginsights.jar:/ngwb/jars/icu4j-53.1.jar:/ngwb/jars/ingest-catalog_2.10-1.0.jar:/ngwb/jars/tika-app-1.10.jar:/ngwb/jars/oms_api-11.5.1.0.200.jar:/ngwb/jars/univocity-parsers-1.5.1.jar'),\n",
       " ('spark.files',\n",
       "  'file:/home/notebook/.ivy2/jars/com.databricks_spark-avro_2.10-1.0.0.jar,file:/home/notebook/.ivy2/jars/org.apache.avro_avro-1.7.6.jar,file:/home/notebook/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:/home/notebook/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:/home/notebook/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:/home/notebook/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar,file:/home/notebook/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:/home/notebook/.ivy2/jars/org.slf4j_slf4j-api-1.6.4.jar,file:/home/notebook/.ivy2/jars/org.tukaani_xz-1.0.jar'),\n",
       " ('spark.jars',\n",
       "  'file:/ngwb/jars/avro-1.7.6.jar,file:/ngwb/jars/oms_client-11.5.1.0.200.jar,file:/ngwb/jars/commons-csv-1.1.jar,file:/ngwb/jars/oms_common-11.5.1.0.200.jar,file:/ngwb/jars/genericds_2.10-1.0.jar,file:/ngwb/jars/simple-regex-4.6.jar,file:/ngwb/jars/htmllexer-2.1.jar,file:/ngwb/jars/spark-avro_2.10-2.0.1.jar,file:/ngwb/jars/htmlparser-2.1.jar,file:/ngwb/jars/spark-csv_2.10-1.3.0.jar,file:/ngwb/jars/ibmprofile_2.10-1.0.jar,file:/ngwb/jars/system-t-runtime-4.6-biginsights.jar,file:/ngwb/jars/icu4j-53.1.jar,file:/ngwb/jars/ingest-catalog_2.10-1.0.jar,file:/ngwb/jars/tika-app-1.10.jar,file:/ngwb/jars/oms_api-11.5.1.0.200.jar,file:/ngwb/jars/univocity-parsers-1.5.1.jar,file:/home/notebook/.ivy2/jars/com.databricks_spark-avro_2.10-1.0.0.jar,file:/home/notebook/.ivy2/jars/org.apache.avro_avro-1.7.6.jar,file:/home/notebook/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:/home/notebook/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:/home/notebook/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:/home/notebook/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar,file:/home/notebook/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:/home/notebook/.ivy2/jars/org.slf4j_slf4j-api-1.6.4.jar,file:/home/notebook/.ivy2/jars/org.tukaani_xz-1.0.jar'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/home/notebook/.ivy2/jars/com.databricks_spark-avro_2.10-1.0.0.jar,/home/notebook/.ivy2/jars/org.apache.avro_avro-1.7.6.jar,/home/notebook/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,/home/notebook/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,/home/notebook/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,/home/notebook/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar,/home/notebook/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,/home/notebook/.ivy2/jars/org.slf4j_slf4j-api-1.6.4.jar,/home/notebook/.ivy2/jars/org.tukaani_xz-1.0.jar'),\n",
       " ('spark.app.name', 'pyspark-shell')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f7ea1684818a'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import socket\n",
    "socket.gethostbyaddr(socket.gethostname())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "h = platform.uname()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f7ea1684818a'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getenv('HOSTNAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 : MapR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read auction data into an RDD\n",
    "auctionRDD = sc.textFile(\"./data/MaprDataFiles/auctiondata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8213034705,95,2.927373,jake7870,0,95,117.5,xbox,3',\n",
       " '8213034705,115,2.943484,davidbresler2,1,95,117.5,xbox,3',\n",
       " '8213034705,100,2.951285,gladimacowgirl,58,95,117.5,xbox,3',\n",
       " '8213034705,117.5,2.998947,daysrus,10,95,117.5,xbox,3',\n",
       " '8213060420,2,0.065266,donnie4814,5,1,120,xbox,3',\n",
       " '8213060420,15.25,0.123218,myreeceyboy,52,1,120,xbox,3',\n",
       " '8213060420,3,0.186539,parakeet2004,5,1,120,xbox,3',\n",
       " '8213060420,10,0.18669,parakeet2004,5,1,120,xbox,3',\n",
       " '8213060420,24.99,0.187049,parakeet2004,5,1,120,xbox,3',\n",
       " '8213060420,20,0.249491,bluebubbles_1,25,1,120,xbox,3']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data in the file by looking at first 10 lines using 'take' command \n",
    "auctionRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you see above output, we can observe that \n",
    "# 1. Output is a list of elements\n",
    "# 2. each line in the file has become an element\n",
    "\n",
    "# This is not usable for our analysis. We need to split each line into seperate fields based on comma \n",
    "# delimiter\n",
    "\n",
    "auctionsRDD = auctionRDD.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['8213034705', '95', '2.927373', 'jake7870', '0', '95', '117.5', 'xbox', '3'],\n",
       " ['8213034705',\n",
       "  '115',\n",
       "  '2.943484',\n",
       "  'davidbresler2',\n",
       "  '1',\n",
       "  '95',\n",
       "  '117.5',\n",
       "  'xbox',\n",
       "  '3'],\n",
       " ['8213034705',\n",
       "  '100',\n",
       "  '2.951285',\n",
       "  'gladimacowgirl',\n",
       "  '58',\n",
       "  '95',\n",
       "  '117.5',\n",
       "  'xbox',\n",
       "  '3']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets take a look at data inside auctionsRDD\n",
    "auctionsRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you see above output, it is a list of lists each list inside the main list represents a line in the file\n",
    "# Each element inside the inner list represents a field or value\n",
    "\n",
    "# Now, declare variables and assign positions so that we can refer various columns in the above RDD using names instead of numbers. \n",
    "# Ex: auctionsRDD[aucid] instead of auctionsRDD[0] \n",
    "# this approach is for best practice as it improves readablility.\n",
    "aucid = 0\n",
    "bid = 1\n",
    "bidtime = 2\n",
    "bidder = 3\n",
    "bidderrate = 4\n",
    "openbid = 5\n",
    "price = 6\n",
    "itemtype = 7\n",
    "dtl = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets explore this dataset more !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8213034705', '95', '2.927373', 'jake7870', '0', '95', '117.5', 'xbox', '3']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. How do you see the first element of the auctionsRDD?\n",
    "auctionsRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['8213034705', '95', '2.927373', 'jake7870', '0', '95', '117.5', 'xbox', '3'],\n",
       " ['8213034705',\n",
       "  '115',\n",
       "  '2.943484',\n",
       "  'davidbresler2',\n",
       "  '1',\n",
       "  '95',\n",
       "  '117.5',\n",
       "  'xbox',\n",
       "  '3'],\n",
       " ['8213034705',\n",
       "  '100',\n",
       "  '2.951285',\n",
       "  'gladimacowgirl',\n",
       "  '58',\n",
       "  '95',\n",
       "  '117.5',\n",
       "  'xbox',\n",
       "  '3'],\n",
       " ['8213034705',\n",
       "  '117.5',\n",
       "  '2.998947',\n",
       "  'daysrus',\n",
       "  '10',\n",
       "  '95',\n",
       "  '117.5',\n",
       "  'xbox',\n",
       "  '3'],\n",
       " ['8213060420', '2', '0.065266', 'donnie4814', '5', '1', '120', 'xbox', '3']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. What do you use to see the first 5 elements of the RDD?\n",
    "auctionsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10654"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. What is the total number of bids?\n",
    "totbids = auctionsRDD.count()\n",
    "totbids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. What is the total number of distinct items that were auctioned?\n",
    "totitems = auctionsRDD.map(lambda auction: auction[aucid]).distinct().count()\n",
    "totitems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. What is the total number of item types that were auctioned?\n",
    "totitemtype = auctionsRDD.map(lambda auction:auction[itemtype]).distinct().count()\n",
    "totitemtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xbox', 2784), ('palm', 5917), ('cartier', 1953)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. What is the total number of bids per item type?\n",
    "bids_itemtype = auctionsRDD.map(lambda auction : (auction[itemtype], 1)).reduceByKey(lambda a,b:a+b)\n",
    "bids_itemtype.collect()\n",
    "#otherways\n",
    "# totitemtype = auctionsRDD.map(lambda auction:(auction[itemtype],1)).countByKey()\n",
    "# totitemtype = auctionsRDD.map(lambda auction:auction[itemtype]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Across all auctioned items, what is the maximum number of bids?\n",
    "maxbids = auctionsRDD.map(lambda auction: (auction[aucid],1)) \\\n",
    "                            .reduceByKey(lambda x,y:x+y) \\\n",
    "                            .map(lambda x: x[1]).reduce(max)\n",
    "maxbids        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Across all auctioned items, what is the minimum number of bids?\n",
    "minbids = auctionsRDD.map(lambda auction: (auction[aucid],1)) \\\n",
    "                            .reduceByKey(lambda x,y:x+y) \\\n",
    "                            .map(lambda x: x[1]).reduce(min)\n",
    "minbids        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.992025518341308"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. What is the average number of bids per auction? Hint: totalbids/totalauctions\n",
    "#totalbids = auctionsRDD.map(lambda auction: (auction[aucid],1)).reduceByKey(lambda x,y:x+y).map(lambda x: x[1]).sum()\n",
    "avgbids = totbids/totitems      \n",
    "avgbids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[38] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. Create an RDD that contains total bids for each auction.\n",
    "bidsAuctionRDD = auctionsRDD.map(lambda auction: (auction[aucid],1)).reduceByKey(lambda x,y:x+y)\n",
    "bidsAuctionRDD.persist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3024579522', 7),\n",
       " ('3018964794', 20),\n",
       " ('3014314236', 5),\n",
       " ('8214279576', 4),\n",
       " ('1643136423', 10)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidsAuctionRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(2) PythonRDD[38] at RDD at PythonRDD.scala:43 [Memory Serialized 1x Replicated]\\n |       CachedPartitions: 1; MemorySize: 4.7 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\\n |  MapPartitionsRDD[37] at mapPartitions at PythonRDD.scala:374 [Memory Serialized 1x Replicated]\\n |  ShuffledRDD[36] at partitionBy at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]\\n +-(2) PairwiseRDD[35] at reduceByKey at <ipython-input-37-0d8d74e87bfd>:2 [Memory Serialized 1x Replicated]\\n    |  PythonRDD[34] at reduceByKey at <ipython-input-37-0d8d74e87bfd>:2 [Memory Serialized 1x Replicated]\\n    |  MapPartitionsRDD[4] at textFile at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]\\n    |  ./data/MaprDataFiles/auctiondata.csv HadoopRDD[3] at textFile at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidsAuctionRDD.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxbids = bidsAuctionRDD.map(lambda bid: bid[1]).reduce(max)\n",
    "maxbids\n",
    "# maxbids = bidsAuctionRDD.map(lambda bid: bid[1]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minbids = bidsAuctionRDD.map(lambda bid: bid[1]).reduce(min)\n",
    "minbids\n",
    "# minbids = bidsAuctionRDD.map(lambda bid: bid[1]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1639672910',\n",
       "  '5200',\n",
       "  '6.291875',\n",
       "  'esmodeus',\n",
       "  '0',\n",
       "  '5000',\n",
       "  '5400',\n",
       "  'cartier',\n",
       "  '7'],\n",
       " ['1639672910',\n",
       "  '5100',\n",
       "  '6.925520833',\n",
       "  'akapson',\n",
       "  '7',\n",
       "  '5000',\n",
       "  '5400',\n",
       "  'cartier',\n",
       "  '7'],\n",
       " ['1639672910',\n",
       "  '5300',\n",
       "  '6.926469907',\n",
       "  'akapson',\n",
       "  '7',\n",
       "  '5000',\n",
       "  '5400',\n",
       "  'cartier',\n",
       "  '7'],\n",
       " ['1639672910',\n",
       "  '5400',\n",
       "  '6.933402778',\n",
       "  'esmodeus',\n",
       "  '0',\n",
       "  '5000',\n",
       "  '5400',\n",
       "  'cartier',\n",
       "  '7']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctionsRDD.filter(lambda x: '1639672910' in x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1639672910', '5400')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxbidamt = auctionsRDD.map(lambda x: (x[0],x[1])).max(key = lambda x : float(x[1]))\n",
    "maxbidamt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6],\n",
       " [7, 8, 9, 10, 11, 12],\n",
       " [13, 14, 15, 16, 17, 18],\n",
       " [19, 20, 21, 22, 23, 24],\n",
       " [25, 26, 27, 28, 29, 30],\n",
       " [31, 32, 33, 34, 35, 36, 37],\n",
       " [38, 39, 40, 41, 42, 43],\n",
       " [44, 45, 46, 47, 48, 49],\n",
       " [50, 51, 52, 53, 54, 55],\n",
       " [56, 57, 58, 59, 60, 61],\n",
       " [62, 63, 64, 65, 66, 67, 68],\n",
       " [69, 70, 71, 72, 73, 74],\n",
       " [75, 76, 77, 78, 79, 80],\n",
       " [81, 82, 83, 84, 85, 86],\n",
       " [87, 88, 89, 90, 91, 92],\n",
       " [93, 94, 95, 96, 97, 98, 99]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COntents of an RDD inside each partition\n",
    "\n",
    "a = sc.parallelize(range(1, 100))\n",
    "# Return an RDD created by coalescing all elements within each partition into a list.\n",
    "a.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.HiveContext at 0x7eff80900eb8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.context.HiveContext"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['8213034705', '95', '2.927373', 'jake7870', '0', '95', '117.5', 'xbox', '3'],\n",
       " ['8213034705',\n",
       "  '115',\n",
       "  '2.943484',\n",
       "  'davidbresler2',\n",
       "  '1',\n",
       "  '95',\n",
       "  '117.5',\n",
       "  'xbox',\n",
       "  '3'],\n",
       " ['8213034705',\n",
       "  '100',\n",
       "  '2.951285',\n",
       "  'gladimacowgirl',\n",
       "  '58',\n",
       "  '95',\n",
       "  '117.5',\n",
       "  'xbox',\n",
       "  '3'],\n",
       " ['8213034705',\n",
       "  '117.5',\n",
       "  '2.998947',\n",
       "  'daysrus',\n",
       "  '10',\n",
       "  '95',\n",
       "  '117.5',\n",
       "  'xbox',\n",
       "  '3'],\n",
       " ['8213060420', '2', '0.065266', 'donnie4814', '5', '1', '120', 'xbox', '3']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a text file and inferring the Schema Using Reflection\n",
    "inputRDD = sc.textFile(\"./data/MaprDataFiles/auctiondata.csv\").map(lambda lines: lines.split(\",\"))\n",
    "inputRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse each line and create a row with proper datatypes\n",
    "auctionsRDD = inputRDD.map(lambda line: Row(aucid = int(line[0]), bid = float(line[1]), bidtime = float(line[2]), \\\n",
    "                                            bidder = line[3], bidderate = int(line[4]), openbid = float(line[5]), \\\n",
    "                                            price = float(line[6]), itemtype = line[7], dtl = int(line[8])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(auctionsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(2) PythonRDD[60] at RDD at PythonRDD.scala:43 []\\n |  MapPartitionsRDD[52] at textFile at NativeMethodAccessorImpl.java:-2 []\\n |  ./data/MaprDataFiles/auctiondata.csv HadoopRDD[51] at textFile at NativeMethodAccessorImpl.java:-2 []'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctionsRDD.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xboxRDD = auctionsRDD.map(lambda line : \"xbox\" in line[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(2) PythonRDD[59] at RDD at PythonRDD.scala:43 []\\n |  MapPartitionsRDD[52] at textFile at NativeMethodAccessorImpl.java:-2 []\\n |  ./data/MaprDataFiles/auctiondata.csv HadoopRDD[51] at textFile at NativeMethodAccessorImpl.java:-2 []'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xboxRDD.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a DataFrame for above RDD\n",
    "auctionsDF = sqlContext.createDataFrame(auctionsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------+---------+--------+---+--------+-------+-----+\n",
      "|     aucid|  bid|        bidder|bidderate| bidtime|dtl|itemtype|openbid|price|\n",
      "+----------+-----+--------------+---------+--------+---+--------+-------+-----+\n",
      "|8213034705| 95.0|      jake7870|        0|2.927373|  3|    xbox|   95.0|117.5|\n",
      "|8213034705|115.0| davidbresler2|        1|2.943484|  3|    xbox|   95.0|117.5|\n",
      "|8213034705|100.0|gladimacowgirl|       58|2.951285|  3|    xbox|   95.0|117.5|\n",
      "|8213034705|117.5|       daysrus|       10|2.998947|  3|    xbox|   95.0|117.5|\n",
      "|8213060420|  2.0|    donnie4814|        5|0.065266|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|15.25|   myreeceyboy|       52|0.123218|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|  3.0|  parakeet2004|        5|0.186539|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 10.0|  parakeet2004|        5| 0.18669|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|24.99|  parakeet2004|        5|0.187049|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 20.0| bluebubbles_1|       25|0.249491|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 22.0| bluebubbles_1|       25| 0.24956|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 24.0| bluebubbles_1|       25|0.249653|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 26.0| bluebubbles_1|       25|0.249757|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 80.0|        sa4741|        3| 0.59059|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 75.0|     jhnsn2273|       51|0.657384|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 85.0|     jhnsn2273|       51|0.657917|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 83.0|        sa4741|        3|0.816447|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|100.0|        sa4741|        3|1.005903|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|100.0|     jhnsn2273|       51|1.012697|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|110.0|     jhnsn2273|       51|1.013056|  3|    xbox|    1.0|120.0|\n",
      "+----------+-----+--------------+---------+--------+---+--------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auctionsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aucid: long (nullable = true)\n",
      " |-- bid: double (nullable = true)\n",
      " |-- bidder: string (nullable = true)\n",
      " |-- bidderate: long (nullable = true)\n",
      " |-- bidtime: double (nullable = true)\n",
      " |-- dtl: long (nullable = true)\n",
      " |-- itemtype: string (nullable = true)\n",
      " |-- openbid: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auctionsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Register dataframe as a table\n",
    "auctionsDF.registerTempTable(\"auctionTbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[62] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auctionTbl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-d6f44a1998c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauctionTbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'auctionTbl' is not defined"
     ]
    }
   ],
   "source": [
    "type(auctionTbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "| tableName|isTemporary|\n",
      "+----------+-----------+\n",
      "|auctiontbl|       true|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10654"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctionsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[aucid: bigint, bid: double, bidder: string, bidderate: bigint, bidtime: double, dtl: bigint, itemtype: string, openbid: double, price: double]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctionsDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, aucid: string, bid: string, bidderate: string, bidtime: string, dtl: string, openbid: string, price: string]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctionsDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aucid',\n",
       " 'bid',\n",
       " 'bidder',\n",
       " 'bidderate',\n",
       " 'bidtime',\n",
       " 'dtl',\n",
       " 'itemtype',\n",
       " 'openbid',\n",
       " 'price']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctionsDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|itemtype|count|\n",
      "+--------+-----+\n",
      "|    xbox| 2784|\n",
      "|    palm| 5917|\n",
      "| cartier| 1953|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auctionsDF.groupby(\"itemtype\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|itemtype|\n",
      "+--------+\n",
      "|    xbox|\n",
      "|    palm|\n",
      "| cartier|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auctionsDF.select(\"itemtype\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets answer few questions about data in this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10654"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. What is the total number of bids?\n",
    "auctionsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. What is the number of distinct auctions?\n",
    "auctionsDF.select(\"aucid\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. What is the number of distinct itemtypes?\n",
    "auctionsDF.select(\"itemtype\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+\n",
      "|itemtype|     aucid|count|\n",
      "+--------+----------+-----+\n",
      "|    palm|3023389524|   16|\n",
      "|    palm|3023746524|   34|\n",
      "|    xbox|8214786481|   16|\n",
      "|    palm|3017911925|    8|\n",
      "| cartier|1642414724|   14|\n",
      "| cartier|1644739924|   18|\n",
      "|    palm|3017841326|   15|\n",
      "|    palm|3024579326|   11|\n",
      "|    xbox|8214418083|   12|\n",
      "|    palm|3013839128|   22|\n",
      "|    xbox|8211763485|   31|\n",
      "|    xbox|8212610485|   10|\n",
      "|    xbox|8212755285|   26|\n",
      "| cartier|1641628327|    7|\n",
      "| cartier|1640936328|   22|\n",
      "|    palm|3019224930|   30|\n",
      "|    palm|3018457930|   29|\n",
      "|    palm|3020454530|   21|\n",
      "|    xbox|8212966887|   12|\n",
      "|    xbox|8214767887|    9|\n",
      "+--------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4. We would like a count of bids per auction and the item type (as shown below). How would you do this?\n",
    "auctionsDF.groupBy(\"itemtype\", \"aucid\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "|Max|Min|             Count|\n",
      "+---+---+------------------+\n",
      "| 75|  1|16.992025518341308|\n",
      "+---+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. For each auction item and item type, we want the max, min and average number of bids.\n",
    "import pyspark.sql.functions as func\n",
    "auctionsDF.groupBy(\"itemtype\",\"aucid\").count().agg(func.max(\"count\").alias(\"Max\"), func.min(\"count\").alias(\"Min\"), func.avg(\"count\").alias(\"Count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+------------------+\n",
      "|itemtype|     aucid|MaximumBid|MinimumBid|            AvgBid|\n",
      "+--------+----------+----------+----------+------------------+\n",
      "|    palm|3023389524|     212.5|      9.99|         111.84375|\n",
      "|    palm|3023746524|     177.0|       1.0| 77.47323529411766|\n",
      "|    xbox|8214786481|     120.0|      44.0|           86.3125|\n",
      "|    palm|3017911925|     290.0|     225.0|             257.5|\n",
      "| cartier|1642414724|     510.0|      12.0|262.57142857142856|\n",
      "| cartier|1644739924|     455.0|       1.0|247.73611111111111|\n",
      "|    palm|3017841326|     238.0|     152.5|197.84466666666665|\n",
      "|    palm|3024579326|     237.5|     190.0| 219.3172727272727|\n",
      "|    xbox|8214418083|     140.5|      75.0|113.71833333333332|\n",
      "|    palm|3013839128|    251.11|      30.0| 133.8577272727273|\n",
      "|    xbox|8211763485|     139.3|      10.0| 67.37806451612903|\n",
      "|    xbox|8212610485|      33.0|       1.0|              21.8|\n",
      "|    xbox|8212755285|     182.5|      10.0|112.21346153846154|\n",
      "| cartier|1641628327|   1894.78|   1599.99|1766.3642857142859|\n",
      "| cartier|1640936328|     317.0|      12.0|199.53500000000003|\n",
      "|    palm|3019224930|     195.1|      15.0|124.29366666666667|\n",
      "|    palm|3018457930|     227.5|      20.0|118.54034482758621|\n",
      "|    palm|3020454530|     227.5|     27.68|132.70190476190479|\n",
      "|    xbox|8212966887|     102.5|      55.0|             88.75|\n",
      "|    xbox|8214767887|    105.01|      75.0| 90.89222222222222|\n",
      "+--------+----------+----------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. For each auction item and item type, we want the max, min and average of a bid:\n",
    "auctionsDF.groupBy(\"itemtype\",\"aucid\").agg(func.max(\"bid\").alias(\"MaximumBid\"), func.min(\"bid\").alias(\"MinimumBid\"), func.avg(\"bid\").alias(\"AvgBid\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7685"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. What is the number of auctions with final price greater than 200?\n",
    "auctionsDF.filter(auctionsDF.price > 200).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------+---------+--------+---+--------+-------+-----+\n",
      "|     aucid|  bid|        bidder|bidderate| bidtime|dtl|itemtype|openbid|price|\n",
      "+----------+-----+--------------+---------+--------+---+--------+-------+-----+\n",
      "|8213034705| 95.0|      jake7870|        0|2.927373|  3|    xbox|   95.0|117.5|\n",
      "|8213034705|115.0| davidbresler2|        1|2.943484|  3|    xbox|   95.0|117.5|\n",
      "|8213034705|100.0|gladimacowgirl|       58|2.951285|  3|    xbox|   95.0|117.5|\n",
      "|8213034705|117.5|       daysrus|       10|2.998947|  3|    xbox|   95.0|117.5|\n",
      "|8213060420|  2.0|    donnie4814|        5|0.065266|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|15.25|   myreeceyboy|       52|0.123218|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|  3.0|  parakeet2004|        5|0.186539|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 10.0|  parakeet2004|        5| 0.18669|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|24.99|  parakeet2004|        5|0.187049|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 20.0| bluebubbles_1|       25|0.249491|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 22.0| bluebubbles_1|       25| 0.24956|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 24.0| bluebubbles_1|       25|0.249653|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 26.0| bluebubbles_1|       25|0.249757|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 80.0|        sa4741|        3| 0.59059|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 75.0|     jhnsn2273|       51|0.657384|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 85.0|     jhnsn2273|       51|0.657917|  3|    xbox|    1.0|120.0|\n",
      "|8213060420| 83.0|        sa4741|        3|0.816447|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|100.0|        sa4741|        3|1.005903|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|100.0|     jhnsn2273|       51|1.012697|  3|    xbox|    1.0|120.0|\n",
      "|8213060420|110.0|     jhnsn2273|       51|1.013056|  3|    xbox|    1.0|120.0|\n",
      "+----------+-----+--------------+---------+--------+---+--------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xboxes = sqlContext.sql(\"select * from auctiontbl where itemtype = 'xbox'\")\n",
    "xboxes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|Auction ID|   bid|\n",
      "+----------+------+\n",
      "|8213935134| 202.5|\n",
      "|8213935134| 205.0|\n",
      "|8213935134| 207.5|\n",
      "|8212830525|386.87|\n",
      "|8212830525| 203.0|\n",
      "|8212830525| 210.0|\n",
      "|8212830525| 216.0|\n",
      "|8212830525| 222.0|\n",
      "|8212830525| 228.0|\n",
      "|8212830525| 239.0|\n",
      "|8212830525| 245.0|\n",
      "|8212830525| 250.0|\n",
      "|8212830525| 260.0|\n",
      "|8212830525| 275.0|\n",
      "|8212830525| 285.0|\n",
      "|8212830525| 300.0|\n",
      "|8212830525| 350.0|\n",
      "|8212830525|403.87|\n",
      "|8212830525| 375.0|\n",
      "|8212830525| 400.0|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auctionsDF.select(auctionsDF.aucid.alias(\"Auction ID\"),auctionsDF.bid ).filter(auctionsDF.bid > 200).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+---+----+\n",
      "|itemtype_dtl|   3|  5|   7|\n",
      "+------------+----+---+----+\n",
      "|        xbox| 553|393|1838|\n",
      "|        palm|1216|869|3832|\n",
      "|     cartier| 250|355|1348|\n",
      "+------------+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auctionsDF.crosstab(\"itemtype\", \"dtl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiple Aggregate operations on the same column of a spark dataframe\n",
    "\n",
    "from pyspark.sql.functions import mean, sum, max, col\n",
    "\n",
    "df = sc.parallelize([(1, 3.0), (1, 3.0), (2, -5.0)]).toDF([\"k\", \"v\"])\n",
    "\n",
    "groupBy = [\"k\"]\n",
    "aggregate = [\"v\"] \n",
    "funs = [mean, sum, max]\n",
    "\n",
    "exprs = [f(col(c)) for f in funs for c in aggregate]\n",
    "\n",
    "# or equivalent df.groupby(groupBy).agg(*exprs)\n",
    "df.groupby(*groupBy).agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Random Data Generation\n",
    "import pyspark.sql.functions as f\n",
    "df = sqlContext.range(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "| id|            uniform|              normal|\n",
      "+---+-------------------+--------------------+\n",
      "|  0| 0.1982919638208397| 0.06157382353970104|\n",
      "|  1|0.44292918521277047| -0.4798519469521663|\n",
      "|  2| 0.2731073068483362|-0.15116027592854422|\n",
      "|  3|0.27149331793166864|-0.18575112254167045|\n",
      "|  4| 0.6037143578435027|   0.734722467897308|\n",
      "|  5|0.20981311586756546|-0.40496097207840054|\n",
      "|  6| 0.4013331117277633|-0.21535632933479554|\n",
      "|  7| 0.8469085276976173|  0.4932212707592389|\n",
      "|  8|  0.819164334221561|  -1.593640432116535|\n",
      "|  9| 0.2626612522319226|  0.4085363219031828|\n",
      "+---+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate two other columns using uniform distribution and normal distribution.\n",
    "df.select(\"id\", f.rand(seed=10).alias(\"uniform\"), f.randn(seed=27).alias(\"normal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A slightly different way to generate the two random columns\n",
    "df = sqlContext.range(0, 10) \\\n",
    "               .withColumn('uniform', f.rand(seed=10)) \\\n",
    "               .withColumn('normal', f.randn(seed=27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+\n",
      "|summary|                id|           uniform|              normal|\n",
      "+-------+------------------+------------------+--------------------+\n",
      "|  count|                10|                10|                  10|\n",
      "|   mean|               4.5|0.4329416473403548|-0.13326671948526814|\n",
      "| stddev|3.0276503540974917|0.2440977761617599|  0.6509693428098151|\n",
      "|    min|                 0|0.1982919638208397|  -1.593640432116535|\n",
      "|    max|                 9|0.8469085276976173|   0.734722467897308|\n",
      "+-------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2. Summary and Descriptive Statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+\n",
      "|summary|           uniform|              normal|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|                10|                  10|\n",
      "|   mean|0.4329416473403548|-0.13326671948526814|\n",
      "| stddev|0.2440977761617599|  0.6509693428098151|\n",
      "|    min|0.1982919638208397|  -1.593640432116535|\n",
      "|    max|0.8469085276976173|   0.734722467897308|\n",
      "+-------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe on a subset of the columns\n",
    "df.describe('uniform', 'normal').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+\n",
      "|      avg(uniform)|      min(uniform)|      max(uniform)|\n",
      "+------------------+------------------+------------------+\n",
      "|0.4329416473403548|0.1982919638208397|0.8469085276976173|\n",
      "+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "df.select(mean('uniform'), min('uniform'), max('uniform')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.024815950987931417"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Sample covariance and correlation.\n",
    "\"\"\"\n",
    "Covariance is a measure of how two variables change with respect to each other. \n",
    "A positive number would mean that there is a tendency that as one variable increases, the other increases as well. \n",
    "A negative number would mean that as one variable increases, the other variable has a tendency to decrease. \n",
    "The sample covariance of two columns of a DataFrame can be calculated as follows:\n",
    "\"\"\"\n",
    "df.stat.cov('uniform', 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.166666666666664"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.cov('id', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15617321967079692"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation is a normalized measure of covariance that is easier to understand,\n",
    "# as it provides quantitative measurements of the statistical dependence between two random variables.\n",
    "df.stat.corr('uniform', 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr('id', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with two columns (name, item) \n",
    "names = [\"Alice\", \"Bob\", \"Mike\"]\n",
    "items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\n",
    "df = sqlContext.createDataFrame([(names[i % 3], items[i % 5]) for i in range(100)], [\"name\", \"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| name|   item|\n",
      "+-----+-------+\n",
      "|Alice|   milk|\n",
      "|  Bob|  bread|\n",
      "| Mike| butter|\n",
      "|Alice| apples|\n",
      "|  Bob|oranges|\n",
      "| Mike|   milk|\n",
      "|Alice|  bread|\n",
      "|  Bob| butter|\n",
      "| Mike| apples|\n",
      "|Alice|oranges|\n",
      "+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 10 rows.\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------+----+-----+\n",
      "|name_item|apples|oranges|butter|milk|bread|\n",
      "+---------+------+-------+------+----+-----+\n",
      "|      Bob|     6|      7|     7|   6|    7|\n",
      "|     Mike|     7|      6|     7|   7|    6|\n",
      "|    Alice|     7|      7|     6|   7|    7|\n",
      "+---------+------+-------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.crosstab(\"name\", \"item\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq = df.stat.freqItems([\"name\",\"item\"],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|name_freqItems|item_freqItems|\n",
      "+--------------+--------------+\n",
      "|       [Alice]|     [oranges]|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplimentary Mapr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Address='JACKSON_ST/POWELL_ST', Category='OTHER_OFFENSES', Date='7/9/15', DayOfWeek='Thursday', Descript='POSSESSION_OF_BURGLARY_TOOLS', IncidentNum=150599321, PdDistrict='CENTRAL', PdID=15059900000000, Resolution='ARREST/BOOKED', Time='23:45', XCoord='-122.4099006', YCoord='37.79561712'),\n",
       " Row(Address='300_Block_of_POWELL_ST', Category='LARCENY/THEFT', Date='7/9/15', DayOfWeek='Thursday', Descript='PETTY_THEFT_OF_PROPERTY', IncidentNum=156168837, PdDistrict='CENTRAL', PdID=15616900000000, Resolution='NONE', Time='23:45', XCoord='-122.4083843', YCoord='37.78782711'),\n",
       " Row(Address='JACKSON_ST/POWELL_ST', Category='OTHER_OFFENSES', Date='7/9/15', DayOfWeek='Thursday', Descript='DRIVERS_LICENSE/SUSPENDED_OR_REVOKED', IncidentNum=150599321, PdDistrict='CENTRAL', PdID=15059900000000, Resolution='ARREST/BOOKED', Time='23:45', XCoord='-122.4099006', YCoord='37.79561712'),\n",
       " Row(Address='MASONIC_AV/GOLDEN_GATE_AV', Category='OTHER_OFFENSES', Date='7/9/15', DayOfWeek='Thursday', Descript='DRIVERS_LICENSE/SUSPENDED_OR_REVOKED', IncidentNum=150599224, PdDistrict='PARK', PdID=15059900000000, Resolution='ARREST/BOOKED', Time='23:36', XCoord='-122.4468469', YCoord='37.77766882'),\n",
       " Row(Address='8TH_ST/FOLSOM_ST', Category='LARCENY/THEFT', Date='7/9/15', DayOfWeek='Thursday', Descript='GRAND_THEFT_FROM_LOCKED_AUTO', IncidentNum=156169067, PdDistrict='SOUTHERN', PdID=15616900000000, Resolution='NONE', Time='23:30', XCoord='-122.4100658', YCoord='37.77499068')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfpdRDD = sc.textFile(\"./data/MaprDataFiles/sfpd.csv\") \\\n",
    "        .map(lambda line:line.split(\",\")) \\\n",
    "        .map(lambda i: Row(IncidentNum = int(i[0]), Category = i[1], Descript = i[2], DayOfWeek = i[3] \\\n",
    "                         , Date = i[4], Time = i[5], PdDistrict = i[6], Resolution = i[7] \\\n",
    "                         , Address = i[8], XCoord = i[9], YCoord = i[10], PdID = int(i[11])))\n",
    "sfpdRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sfpdDF = sqlContext.createDataFrame(sfpdRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- IncidentNum: long (nullable = true)\n",
      " |-- PdDistrict: string (nullable = true)\n",
      " |-- PdID: long (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- XCoord: string (nullable = true)\n",
      " |-- YCoord: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfpdDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfpdDF.registerTempTable(\"sfpdtbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Address='JACKSON_ST/POWELL_ST', Category='OTHER_OFFENSES', Date='7/9/15', DayOfWeek='Thursday', Descript='POSSESSION_OF_BURGLARY_TOOLS', IncidentNum=150599321, PdDistrict='CENTRAL', PdID=15059900000000, Resolution='ARREST/BOOKED', Time='23:45', XCoord='-122.4099006', YCoord='37.79561712'),\n",
       " Row(Address='300_Block_of_POWELL_ST', Category='LARCENY/THEFT', Date='7/9/15', DayOfWeek='Thursday', Descript='PETTY_THEFT_OF_PROPERTY', IncidentNum=156168837, PdDistrict='CENTRAL', PdID=15616900000000, Resolution='NONE', Time='23:45', XCoord='-122.4083843', YCoord='37.78782711'),\n",
       " Row(Address='JACKSON_ST/POWELL_ST', Category='OTHER_OFFENSES', Date='7/9/15', DayOfWeek='Thursday', Descript='DRIVERS_LICENSE/SUSPENDED_OR_REVOKED', IncidentNum=150599321, PdDistrict='CENTRAL', PdID=15059900000000, Resolution='ARREST/BOOKED', Time='23:45', XCoord='-122.4099006', YCoord='37.79561712'),\n",
       " Row(Address='MASONIC_AV/GOLDEN_GATE_AV', Category='OTHER_OFFENSES', Date='7/9/15', DayOfWeek='Thursday', Descript='DRIVERS_LICENSE/SUSPENDED_OR_REVOKED', IncidentNum=150599224, PdDistrict='PARK', PdID=15059900000000, Resolution='ARREST/BOOKED', Time='23:36', XCoord='-122.4468469', YCoord='37.77766882'),\n",
       " Row(Address='8TH_ST/FOLSOM_ST', Category='LARCENY/THEFT', Date='7/9/15', DayOfWeek='Thursday', Descript='GRAND_THEFT_FROM_LOCKED_AUTO', IncidentNum=156169067, PdDistrict='SOUTHERN', PdID=15616900000000, Resolution='NONE', Time='23:30', XCoord='-122.4100658', YCoord='37.77499068')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seeing the first few lines of the DataFrame\n",
    "\n",
    "sfpdDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------+---------+--------------------+-----------+----------+--------------+-------------+-----+------------+-----------+\n",
      "|             Address|      Category|  Date|DayOfWeek|            Descript|IncidentNum|PdDistrict|          PdID|   Resolution| Time|      XCoord|     YCoord|\n",
      "+--------------------+--------------+------+---------+--------------------+-----------+----------+--------------+-------------+-----+------------+-----------+\n",
      "|JACKSON_ST/POWELL_ST|OTHER_OFFENSES|7/9/15| Thursday|POSSESSION_OF_BUR...|  150599321|   CENTRAL|15059900000000|ARREST/BOOKED|23:45|-122.4099006|37.79561712|\n",
      "|300_Block_of_POWE...| LARCENY/THEFT|7/9/15| Thursday|PETTY_THEFT_OF_PR...|  156168837|   CENTRAL|15616900000000|         NONE|23:45|-122.4083843|37.78782711|\n",
      "|JACKSON_ST/POWELL_ST|OTHER_OFFENSES|7/9/15| Thursday|DRIVERS_LICENSE/S...|  150599321|   CENTRAL|15059900000000|ARREST/BOOKED|23:45|-122.4099006|37.79561712|\n",
      "|MASONIC_AV/GOLDEN...|OTHER_OFFENSES|7/9/15| Thursday|DRIVERS_LICENSE/S...|  150599224|      PARK|15059900000000|ARREST/BOOKED|23:36|-122.4468469|37.77766882|\n",
      "|    8TH_ST/FOLSOM_ST| LARCENY/THEFT|7/9/15| Thursday|GRAND_THEFT_FROM_...|  156169067|  SOUTHERN|15616900000000|         NONE|23:30|-122.4100658|37.77499068|\n",
      "+--------------------+--------------+------+---------+--------------------+-----------+----------+--------------+-------------+-----+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfpdDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Address='JACKSON_ST/POWELL_ST', Category='OTHER_OFFENSES', Date='7/9/15', DayOfWeek='Thursday', Descript='POSSESSION_OF_BURGLARY_TOOLS', IncidentNum=150599321, PdDistrict='CENTRAL', PdID=15059900000000, Resolution='ARREST/BOOKED', Time='23:45', XCoord='-122.4099006', YCoord='37.79561712'),\n",
       " Row(Address='300_Block_of_POWELL_ST', Category='LARCENY/THEFT', Date='7/9/15', DayOfWeek='Thursday', Descript='PETTY_THEFT_OF_PROPERTY', IncidentNum=156168837, PdDistrict='CENTRAL', PdID=15616900000000, Resolution='NONE', Time='23:45', XCoord='-122.4083843', YCoord='37.78782711'),\n",
       " Row(Address='JACKSON_ST/POWELL_ST', Category='OTHER_OFFENSES', Date='7/9/15', DayOfWeek='Thursday', Descript='DRIVERS_LICENSE/SUSPENDED_OR_REVOKED', IncidentNum=150599321, PdDistrict='CENTRAL', PdID=15059900000000, Resolution='ARREST/BOOKED', Time='23:45', XCoord='-122.4099006', YCoord='37.79561712'),\n",
       " Row(Address='MASONIC_AV/GOLDEN_GATE_AV', Category='OTHER_OFFENSES', Date='7/9/15', DayOfWeek='Thursday', Descript='DRIVERS_LICENSE/SUSPENDED_OR_REVOKED', IncidentNum=150599224, PdDistrict='PARK', PdID=15059900000000, Resolution='ARREST/BOOKED', Time='23:36', XCoord='-122.4468469', YCoord='37.77766882'),\n",
       " Row(Address='8TH_ST/FOLSOM_ST', Category='LARCENY/THEFT', Date='7/9/15', DayOfWeek='Thursday', Descript='GRAND_THEFT_FROM_LOCKED_AUTO', IncidentNum=156169067, PdDistrict='SOUTHERN', PdID=15616900000000, Resolution='NONE', Time='23:30', XCoord='-122.4100658', YCoord='37.77499068')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfpdDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------+---------+--------------------+-----------+----------+--------------+-------------+-----+------------+-----------+\n",
      "|             Address|      Category|  Date|DayOfWeek|            Descript|IncidentNum|PdDistrict|          PdID|   Resolution| Time|      XCoord|     YCoord|\n",
      "+--------------------+--------------+------+---------+--------------------+-----------+----------+--------------+-------------+-----+------------+-----------+\n",
      "|JACKSON_ST/POWELL_ST|OTHER_OFFENSES|7/9/15| Thursday|POSSESSION_OF_BUR...|  150599321|   CENTRAL|15059900000000|ARREST/BOOKED|23:45|-122.4099006|37.79561712|\n",
      "|300_Block_of_POWE...| LARCENY/THEFT|7/9/15| Thursday|PETTY_THEFT_OF_PR...|  156168837|   CENTRAL|15616900000000|         NONE|23:45|-122.4083843|37.78782711|\n",
      "|JACKSON_ST/POWELL_ST|OTHER_OFFENSES|7/9/15| Thursday|DRIVERS_LICENSE/S...|  150599321|   CENTRAL|15059900000000|ARREST/BOOKED|23:45|-122.4099006|37.79561712|\n",
      "|MASONIC_AV/GOLDEN...|OTHER_OFFENSES|7/9/15| Thursday|DRIVERS_LICENSE/S...|  150599224|      PARK|15059900000000|ARREST/BOOKED|23:36|-122.4468469|37.77766882|\n",
      "|    8TH_ST/FOLSOM_ST| LARCENY/THEFT|7/9/15| Thursday|GRAND_THEFT_FROM_...|  156169067|  SOUTHERN|15616900000000|         NONE|23:30|-122.4100658|37.77499068|\n",
      "+--------------------+--------------+------+---------+--------------------+-----------+----------+--------------+-------------+-----+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from sfpdtbl limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- IncidentNum: long (nullable = true)\n",
      " |-- PdDistrict: string (nullable = true)\n",
      " |-- PdID: long (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- XCoord: string (nullable = true)\n",
      " |-- YCoord: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in a tree format\n",
    "sfpdDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Category|\n",
      "+--------------------+\n",
      "|      OTHER_OFFENSES|\n",
      "|       VEHICLE_THEFT|\n",
      "|             RUNAWAY|\n",
      "|               FRAUD|\n",
      "|            TRESPASS|\n",
      "|           LOITERING|\n",
      "|SEX_OFFENSES/NON_...|\n",
      "|            BURGLARY|\n",
      "|DRIVING_UNDER_THE...|\n",
      "|       LARCENY/THEFT|\n",
      "|  DISORDERLY_CONDUCT|\n",
      "|      MISSING_PERSON|\n",
      "|         WEAPON_LAWS|\n",
      "|         LIQUOR_LAWS|\n",
      "|             ROBBERY|\n",
      "|   RECOVERED_VEHICLE|\n",
      "|        PROSTITUTION|\n",
      "|     STOLEN_PROPERTY|\n",
      "|          KIDNAPPING|\n",
      "|             SUICIDE|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all the categories on incidents\n",
    "sfpdDF.select(\"Category\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Category|\n",
      "+--------------------+\n",
      "|      OTHER_OFFENSES|\n",
      "|       VEHICLE_THEFT|\n",
      "|             RUNAWAY|\n",
      "|               FRAUD|\n",
      "|            TRESPASS|\n",
      "|           LOITERING|\n",
      "|SEX_OFFENSES/NON_...|\n",
      "|            BURGLARY|\n",
      "|DRIVING_UNDER_THE...|\n",
      "|       LARCENY/THEFT|\n",
      "|  DISORDERLY_CONDUCT|\n",
      "|      MISSING_PERSON|\n",
      "|         WEAPON_LAWS|\n",
      "|         LIQUOR_LAWS|\n",
      "|             ROBBERY|\n",
      "|   RECOVERED_VEHICLE|\n",
      "|        PROSTITUTION|\n",
      "|     STOLEN_PROPERTY|\n",
      "|          KIDNAPPING|\n",
      "|             SUICIDE|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "               SELECT DISTINCT Category \n",
    "                 FROM sfpdtbl\n",
    "               \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Category|\n",
      "+--------------------+\n",
      "|      OTHER_OFFENSES|\n",
      "|       VEHICLE_THEFT|\n",
      "|             RUNAWAY|\n",
      "|               FRAUD|\n",
      "|            TRESPASS|\n",
      "|           LOITERING|\n",
      "|SEX_OFFENSES/NON_...|\n",
      "|            BURGLARY|\n",
      "|DRIVING_UNDER_THE...|\n",
      "|       LARCENY/THEFT|\n",
      "|  DISORDERLY_CONDUCT|\n",
      "|      MISSING_PERSON|\n",
      "|         WEAPON_LAWS|\n",
      "|         LIQUOR_LAWS|\n",
      "|             ROBBERY|\n",
      "|   RECOVERED_VEHICLE|\n",
      "|        PROSTITUTION|\n",
      "|     STOLEN_PROPERTY|\n",
      "|          KIDNAPPING|\n",
      "|             SUICIDE|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfpdDF.select(sfpdDF['Category']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|PdDistrict|\n",
      "+----------+\n",
      "| INGLESIDE|\n",
      "|  RICHMOND|\n",
      "|      PARK|\n",
      "|  NORTHERN|\n",
      "|   TARAVAL|\n",
      "|   CENTRAL|\n",
      "|   BAYVIEW|\n",
      "|   MISSION|\n",
      "|TENDERLOIN|\n",
      "|  SOUTHERN|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all the districts\n",
    "sfpdDF.select(\"PdDistrict\").alias(\"District\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|   Category|IncidentNum|\n",
      "+-----------+-----------+\n",
      "|WEAPON_LAWS|  140274812|\n",
      "|WEAPON_LAWS|  140262180|\n",
      "|WEAPON_LAWS|  140274715|\n",
      "|WEAPON_LAWS|  140275898|\n",
      "|WEAPON_LAWS|  140274561|\n",
      "|WEAPON_LAWS|  140268241|\n",
      "|WEAPON_LAWS|  140263570|\n",
      "|WEAPON_LAWS|  140268213|\n",
      "|WEAPON_LAWS|  140272139|\n",
      "|WEAPON_LAWS|  140268213|\n",
      "|WEAPON_LAWS|  140272117|\n",
      "|WEAPON_LAWS|  140267390|\n",
      "|WEAPON_LAWS|  140272048|\n",
      "|WEAPON_LAWS|  140264540|\n",
      "|WEAPON_LAWS|  140269045|\n",
      "|WEAPON_LAWS|  140267083|\n",
      "|WEAPON_LAWS|  140277333|\n",
      "|WEAPON_LAWS|  140266637|\n",
      "|WEAPON_LAWS|  140272117|\n",
      "|WEAPON_LAWS|  140266079|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all the incidents by category and sort by category desc\n",
    "sfpdDF.select(\"Category\", \"IncidentNum\").sort(sfpdDF['Category'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|Category|IncidentNum|\n",
      "+--------+-----------+\n",
      "|   ARSON|  140269186|\n",
      "|   ARSON|  140149726|\n",
      "|   ARSON|  140267130|\n",
      "|   ARSON|  140206984|\n",
      "|   ARSON|  140187710|\n",
      "|   ARSON|  140275810|\n",
      "|   ARSON|  140187005|\n",
      "|   ARSON|  140256230|\n",
      "|   ARSON|  140172197|\n",
      "|   ARSON|  140241394|\n",
      "|   ARSON|  140169376|\n",
      "|   ARSON|  140226578|\n",
      "|   ARSON|  140168475|\n",
      "|   ARSON|  140229362|\n",
      "|   ARSON|  140166855|\n",
      "|   ARSON|  140268188|\n",
      "|   ARSON|  140161786|\n",
      "|   ARSON|  140234719|\n",
      "|   ARSON|  140158783|\n",
      "|   ARSON|  140155632|\n",
      "+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT Category, IncidentNum from SFPDtbl order by Category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|   Category|IncidentNum|\n",
      "+-----------+-----------+\n",
      "|WEAPON_LAWS|  150592228|\n",
      "|WEAPON_LAWS|  150581732|\n",
      "|WEAPON_LAWS|  150592228|\n",
      "|WEAPON_LAWS|  150598652|\n",
      "|WEAPON_LAWS|  150592228|\n",
      "|WEAPON_LAWS|  150598351|\n",
      "|WEAPON_LAWS|  150588926|\n",
      "|WEAPON_LAWS|  150583368|\n",
      "|WEAPON_LAWS|  150583368|\n",
      "|WEAPON_LAWS|  150597626|\n",
      "|WEAPON_LAWS|  150583368|\n",
      "|WEAPON_LAWS|  150594133|\n",
      "|WEAPON_LAWS|  150583368|\n",
      "|WEAPON_LAWS|  150586839|\n",
      "|WEAPON_LAWS|  150583368|\n",
      "|WEAPON_LAWS|  150588744|\n",
      "|WEAPON_LAWS|  150583211|\n",
      "|WEAPON_LAWS|  150583681|\n",
      "|WEAPON_LAWS|  150583017|\n",
      "|WEAPON_LAWS|  150598674|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "sfpdDF.select(\"Category\", \"IncidentNum\").sort(col(\"Category\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+-------+-------+----+--------+---------+----------+--------+-------+\n",
      "| Category_PdDistrict|SOUTHERN|MISSION|BAYVIEW|TARAVAL|PARK|NORTHERN|INGLESIDE|TENDERLOIN|RICHMOND|CENTRAL|\n",
      "+--------------------+--------+-------+-------+-------+----+--------+---------+----------+--------+-------+\n",
      "|      MISSING_PERSON|    1764|   1739|   1469|   1191|1370|     826|     1430|       448|     584|    739|\n",
      "|           VANDALISM|    3109|   2045|   2008|   1653|1042|    2081|     1908|       787|    1189|   2165|\n",
      "|                TREA|       1|      1|      1|      2|   0|       1|        0|         0|       0|      0|\n",
      "|         LIQUOR_LAWS|      93|     86|     43|     47|  36|      31|       26|        70|      28|     34|\n",
      "|      SUSPICIOUS_OCC|    2364|   1755|   1675|   1109| 668|    1410|     1255|      1225|     929|   1269|\n",
      "|          BAD CHECKS|      15|      8|      5|     10|   2|      10|        3|         1|       6|      9|\n",
      "|PORNOGRAPHY/OBSCE...|       1|      0|      2|      1|   1|       0|        2|         0|       2|      1|\n",
      "|             BRIBERY|      26|     37|     33|      5|   6|       8|       19|         8|       6|     11|\n",
      "|            TRESPASS|     574|    447|    208|    184| 129|     378|      141|       310|     102|    457|\n",
      "|   RECOVERED_VEHICLE|      82|     77|    157|     67|  25|      95|      131|        41|      35|     50|\n",
      "|           LOITERING|      23|     22|      5|      4|   1|      21|        3|        21|       1|      7|\n",
      "|         WEAPON_LAWS|     563|    738|    851|    196| 128|     341|      422|       384|     118|    234|\n",
      "|            GAMBLING|       1|      9|     13|      2|   0|       1|       10|         4|       0|      6|\n",
      "|             RUNAWAY|      37|    106|     46|    119| 102|      17|       52|         4|      22|     16|\n",
      "|               FRAUD|    1325|   1004|    368|    679| 380|     852|      524|       553|     459|   1272|\n",
      "|      OTHER_OFFENSES|    8396|   7272|   6752|   3991|2756|    5034|     5473|      4228|    2627|   4082|\n",
      "|        EMBEZZLEMENT|     106|     35|     45|     23|   6|      49|       14|        35|      16|     63|\n",
      "|       VEHICLE_THEFT|    1435|   2405|   2312|   1771|1212|    2091|     3478|       314|    1254|   1309|\n",
      "|       DRUG/NARCOTIC|    2578|   2190|   1270|    406| 845|    1579|      777|      3576|     367|    712|\n",
      "|SEX_OFFENSES/FORC...|     358|    364|    167|    151|  99|     240|      185|       161|     105|    213|\n",
      "+--------------------+--------+-------+-------+-------+----+--------+---------+----------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counts of all the incidents by category and district\n",
    "sfpdDF.crosstab(\"Category\", \"PdDistrict\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+------+------+--------+------+-------+\n",
      "|  Category_DayOfWeek|Thursday|Wednesday|Friday|Monday|Saturday|Sunday|Tuesday|\n",
      "+--------------------+--------+---------+------+------+--------+------+-------+\n",
      "|      MISSING_PERSON|    1587|     1665|  2100|  1607|    1645|  1343|   1613|\n",
      "|           VANDALISM|    2394|     2426|  2804|  2409|    2904|  2674|   2376|\n",
      "|                TREA|       0|        1|     0|     3|       1|     0|      1|\n",
      "|         LIQUOR_LAWS|      70|       74|    93|    53|      76|    66|     62|\n",
      "|      SUSPICIOUS_OCC|    1999|     2065|  2164|  1889|    1786|  1765|   1991|\n",
      "|          BAD CHECKS|      16|       10|    10|    18|       3|     3|      9|\n",
      "|PORNOGRAPHY/OBSCE...|       0|        2|     1|     2|       1|     1|      3|\n",
      "|             BRIBERY|      18|       29|    26|    25|      27|    21|     13|\n",
      "|            TRESPASS|     436|      433|   417|   430|     388|   384|    442|\n",
      "|   RECOVERED_VEHICLE|     121|      119|   114|   117|      76|    90|    123|\n",
      "|           LOITERING|      19|       14|    13|     9|       9|    10|     34|\n",
      "|         WEAPON_LAWS|     555|      604|   603|   542|     620|   543|    508|\n",
      "|            GAMBLING|      11|        8|    10|     3|       6|     2|      6|\n",
      "|             RUNAWAY|      66|       79|    77|    91|      67|    59|     82|\n",
      "|               FRAUD|    1018|     1118|  1171|  1110|    1048|   899|   1052|\n",
      "|      OTHER_OFFENSES|    7199|     7529|  7782|  7194|    7020|  6335|   7552|\n",
      "|        EMBEZZLEMENT|      52|       57|    64|    80|      41|    38|     60|\n",
      "|       VEHICLE_THEFT|    2427|     2508|  2778|  2425|    2575|  2480|   2388|\n",
      "|       DRUG/NARCOTIC|    2299|     2296|  1981|  2021|    1734|  1725|   2244|\n",
      "|SEX_OFFENSES/FORC...|     278|      267|   325|   277|     337|   315|    244|\n",
      "+--------------------+--------+---------+------+------+--------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfpdDF.crosstab(\"Category\", \"DayOfWeek\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------+------+--------+-------+------+--------+\n",
      "|PdDistrict_DayOfWeek|Wednesday|Monday|Friday|Saturday|Tuesday|Sunday|Thursday|\n",
      "+--------------------+---------+------+------+--------+-------+------+--------+\n",
      "|            SOUTHERN|    10353|  9905| 11608|   11093|  10276|  9619|   10454|\n",
      "|           INGLESIDE|     4819|  4770|  5026|    4635|   4815|  4344|    4750|\n",
      "|          TENDERLOIN|     4597|  4341|  4200|    4169|   4457|  3916|    4494|\n",
      "|             MISSION|     7129|  6938|  7606|    7251|   7103|  7053|    7084|\n",
      "|             TARAVAL|     4137|  4060|  4311|    3751|   4012|  3462|    3737|\n",
      "|            RICHMOND|     3123|  3004|  3282|    3007|   2950|  2843|    3012|\n",
      "|            NORTHERN|     6463|  6235|  7363|    7230|   6399|  6613|    6574|\n",
      "|                PARK|     3545|  3260|  3515|    3276|   3239|  3162|    3380|\n",
      "|             CENTRAL|     5921|  5487|  6621|    6840|   5349|  5984|    5712|\n",
      "|             BAYVIEW|     5355|  5037|  5655|    4986|   5190|  4894|    4994|\n",
      "+--------------------+---------+------+------+--------+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfpdDF.crosstab(\"PdDistrict\",\"DayOfWeek\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+-----+\n",
      "|PdDistrict|DayOfWeek|     Category|count|\n",
      "+----------+---------+-------------+-----+\n",
      "|  SOUTHERN| Saturday|LARCENY/THEFT| 4107|\n",
      "|  SOUTHERN|   Friday|LARCENY/THEFT| 3997|\n",
      "|  SOUTHERN| Thursday|LARCENY/THEFT| 3401|\n",
      "|  SOUTHERN|   Sunday|LARCENY/THEFT| 3365|\n",
      "|  SOUTHERN|Wednesday|LARCENY/THEFT| 3321|\n",
      "|  SOUTHERN|   Monday|LARCENY/THEFT| 3254|\n",
      "|  SOUTHERN|  Tuesday|LARCENY/THEFT| 3208|\n",
      "|  NORTHERN| Saturday|LARCENY/THEFT| 2818|\n",
      "|  NORTHERN|   Friday|LARCENY/THEFT| 2575|\n",
      "|   CENTRAL| Saturday|LARCENY/THEFT| 2322|\n",
      "|  NORTHERN|   Sunday|LARCENY/THEFT| 2309|\n",
      "|   CENTRAL|   Friday|LARCENY/THEFT| 2202|\n",
      "|  NORTHERN| Thursday|LARCENY/THEFT| 2186|\n",
      "|   CENTRAL|Wednesday|LARCENY/THEFT| 2128|\n",
      "|  NORTHERN|Wednesday|LARCENY/THEFT| 2080|\n",
      "|  NORTHERN|   Monday|LARCENY/THEFT| 2078|\n",
      "|  NORTHERN|  Tuesday|LARCENY/THEFT| 2019|\n",
      "|   CENTRAL|   Sunday|LARCENY/THEFT| 2009|\n",
      "|   CENTRAL| Thursday|LARCENY/THEFT| 1988|\n",
      "|   CENTRAL|  Tuesday|LARCENY/THEFT| 1873|\n",
      "+----------+---------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfpdDF.cube(\"PdDistrict\", \"DayOfWeek\", \"Category\").count().dropna().sort(col(\"count\").desc()).show()\n",
    "# .sort(col(\"PdDistrict\"), col(\"DayOfWeek\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many categories are there?\n",
    "sfpdDF.select(\"Category\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table not found: sfpd; line 1 pos 30'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/notebook/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/notebook/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o377.sql.\n: org.apache.spark.sql.AnalysisException: Table not found: sfpd; line 1 pos 30\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$9.applyOrElse(Analyzer.scala:315)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$9.applyOrElse(Analyzer.scala:310)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:53)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:310)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:300)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:36)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:36)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:34)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:52)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:817)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-f36faf624444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT distinct Category FROM sfpd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/notebook/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/notebook/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/notebook/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     49\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table not found: sfpd; line 1 pos 30'"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT distinct Category FROM sfpdtbal\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|          resolution|rescount|\n",
      "+--------------------+--------+\n",
      "|                NONE|  243538|\n",
      "|       ARREST/BOOKED|   86766|\n",
      "|        ARREST/CITED|   22925|\n",
      "|   PSYCHOPATHIC_CASE|    8344|\n",
      "|             LOCATED|    6878|\n",
      "|           UNFOUNDED|    4551|\n",
      "|COMPLAINANT_REFUS...|    4215|\n",
      "|     JUVENILE_BOOKED|    2381|\n",
      "|EXCEPTIONAL_CLEAR...|    1134|\n",
      "|      JUVENILE_CITED|    1039|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What are the top 10 Resolutions ?\n",
    "sqlContext.sql(\"SELECT resolution, count(1) as rescount from sfpd group by resolution order by rescount desc limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|      category|inccount|\n",
      "+--------------+--------+\n",
      "| LARCENY/THEFT|   96955|\n",
      "|OTHER_OFFENSES|   50611|\n",
      "|  NON-CRIMINAL|   50269|\n",
      "|       ASSAULT|   31843|\n",
      "|     VANDALISM|   17987|\n",
      "| VEHICLE_THEFT|   17581|\n",
      "|      WARRANTS|   17508|\n",
      "|      BURGLARY|   15374|\n",
      "| DRUG/NARCOTIC|   14300|\n",
      "|SUSPICIOUS_OCC|   13659|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What are the top 10 most incident Categories?\n",
    "sqlContext.sql(\"SELECT category, count(1) as inccount from sfpd group by category order by inccount desc limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
